{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6228bc1c",
   "metadata": {
    "id": "xSkgt1zf-raF",
    "outputId": "59f4a52f-5eb4-41e5-9fba-07432989fe78",
    "papermill": {
     "duration": 5.620104,
     "end_time": "2025-05-21T16:23:24.361690",
     "exception": false,
     "start_time": "2025-05-21T16:23:18.741586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install matplotlib torch_geometric torch gdown --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e004e5-c0fa-41f4-9be2-1093c8d66aa2",
   "metadata": {
    "id": "5oR2D2Us-xSQ",
    "outputId": "7086cadf-a7fe-4d75-f271-6339bee8164d",
    "papermill": {
     "duration": 4.049235,
     "end_time": "2025-05-21T16:23:28.415556",
     "exception": false,
     "start_time": "2025-05-21T16:23:24.366321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone --branch baselineCe https://github.com/Graph-Classification-Noisy-Label/hackaton.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c129091c",
   "metadata": {
    "id": "tEhfPly6-7UK",
    "outputId": "3078ee06-6312-4fca-f5f9-888fa628c80a",
    "papermill": {
     "duration": 0.013251,
     "end_time": "2025-05-21T16:23:28.434119",
     "exception": false,
     "start_time": "2025-05-21T16:23:28.420868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/onyxia/work/DL-Hackathon/hackaton\n"
     ]
    }
   ],
   "source": [
    "%cd hackaton/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966bdca8-b2a7-4045-83c5-eb9458904c18",
   "metadata": {
    "id": "PxBvwB0_6xI8",
    "outputId": "5933387c-2cfb-474f-d842-f36a3e2d2a73",
    "papermill": {
     "duration": 83.957666,
     "end_time": "2025-05-21T16:24:52.396720",
     "exception": false,
     "start_time": "2025-05-21T16:23:28.439054",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gdown --folder https://drive.google.com/drive/folders/1Z-1JkPJ6q4C6jX4brvq1VRbJH5RPUCAk -O datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80dab1bc",
   "metadata": {
    "id": "1rockhiQ7Nny",
    "outputId": "2cd2e6f4-5f8f-4a62-f0ec-53e6fc78c9b7",
    "papermill": {
     "duration": 0.138969,
     "end_time": "2025-05-21T16:24:52.549260",
     "exception": false,
     "start_time": "2025-05-21T16:24:52.410291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16K\n",
      "drwxr-sr-x 2 onyxia users 4.0K May 30 14:39 A\n",
      "drwxr-sr-x 2 onyxia users 4.0K May 30 14:39 B\n",
      "drwxr-sr-x 2 onyxia users 4.0K May 30 14:40 C\n",
      "drwxr-sr-x 2 onyxia users 4.0K May 30 14:40 D\n"
     ]
    }
   ],
   "source": [
    "!ls -lh datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "817b1078",
   "metadata": {
    "id": "lAQuCuIoBbq5",
    "papermill": {
     "duration": 9.949638,
     "end_time": "2025-05-21T16:25:02.510764",
     "exception": false,
     "start_time": "2025-05-21T16:24:52.561126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "# Load utility functions from cloned repository\n",
    "from src.loadData import GraphDataset\n",
    "from src.utils import set_seed\n",
    "from src.models import GNN\n",
    "import argparse\n",
    "\n",
    "# Set the random seed\n",
    "set_seed()\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fc6f726-09e8-4e45-8d64-78f97ba07c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modification og the class GraphDataset to get indexes\n",
    "\n",
    "#import gzip\n",
    "#import json\n",
    "#import torch\n",
    "#from torch_geometric.data import Dataset, Data\n",
    "#from torch_geometric.loader import DataLoader\n",
    "\n",
    "#class GraphDataset(Dataset):\n",
    "#    def __init__(self, filename, transform=None, pre_transform=None):\n",
    "#        self.raw = filename\n",
    "#        self.num_graphs, self.graphs_dicts = self._count_graphs() \n",
    "#        super().__init__(None, transform, pre_transform)\n",
    "\n",
    "#    def len(self):\n",
    "#        return self.num_graphs  \n",
    "\n",
    "#    # Addition of this function\n",
    "#    def get(self, idx):\n",
    "#        data = dictToGraphObject(self.graphs_dicts[idx])\n",
    "#        data.idx = idx\n",
    "#        return data\n",
    "\n",
    "#    def _count_graphs(self):\n",
    "#        with gzip.open(self.raw, \"rt\", encoding=\"utf-8\") as f:\n",
    "#            graphs_dicts = json.load(f)\n",
    "#            return len(graphs_dicts), graphs_dicts\n",
    "\n",
    "#def dictToGraphObject(graph_dict):\n",
    "#    edge_index = torch.tensor(graph_dict[\"edge_index\"], dtype=torch.long)\n",
    "#    edge_attr = torch.tensor(graph_dict[\"edge_attr\"], dtype=torch.float) if graph_dict[\"edge_attr\"] else None\n",
    "#    num_nodes = graph_dict[\"num_nodes\"]\n",
    "#    y = torch.tensor(graph_dict[\"y\"][0], dtype=torch.long) if graph_dict[\"y\"] is not None else None\n",
    "#    return Data(edge_index=edge_index, edge_attr=edge_attr, num_nodes=num_nodes, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7a760df-1155-424e-a1f9-b7ff93b4d937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to use this one \n",
    "import gzip\n",
    "import json\n",
    "import torch\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, filename, transform=None, pre_transform=None):\n",
    "        self.raw = filename\n",
    "        self.graphs = self.loadGraphs(self.raw)\n",
    "        super().__init__(None, transform, pre_transform)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.graphs[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def loadGraphs(path):\n",
    "        print(f\"Loading graphs from {path}...\")\n",
    "        print(\"This may take a few minutes, please wait...\")\n",
    "        with gzip.open(path, \"rt\", encoding=\"utf-8\") as f:\n",
    "            graphs_dicts = json.load(f)\n",
    "        graphs = []\n",
    "        for graph_dict in tqdm(graphs_dicts, desc=\"Processing graphs\", unit=\"graph\"):\n",
    "            graphs.append(dictToGraphObject(graph_dict))\n",
    "        return graphs\n",
    "\n",
    "\n",
    "\n",
    "def dictToGraphObject(graph_dict):\n",
    "    edge_index = torch.tensor(graph_dict[\"edge_index\"], dtype=torch.long)\n",
    "    edge_attr = torch.tensor(graph_dict[\"edge_attr\"], dtype=torch.float) if graph_dict[\"edge_attr\"] else None\n",
    "    num_nodes = graph_dict[\"num_nodes\"]\n",
    "    y = torch.tensor(graph_dict[\"y\"][0], dtype=torch.long) if graph_dict[\"y\"] is not None else None\n",
    "    return Data(edge_index=edge_index, edge_attr=edge_attr, num_nodes=num_nodes, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0361871",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modification of the class GNN to get the embeddings, in order to build means of class embeddings\n",
    "\n",
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import global_add_pool, global_mean_pool, global_max_pool, GlobalAttention, Set2Set\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.inits import uniform\n",
    "\n",
    "from src.conv import GNN_node, GNN_node_Virtualnode\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_class, num_layer = 5, emb_dim = 300, \n",
    "                    gnn_type = 'gin', virtual_node = True, residual = False, drop_ratio = 0.5, JK = \"last\", graph_pooling = \"mean\"):\n",
    "        '''\n",
    "            num_tasks (int): number of labels to be predicted\n",
    "            virtual_node (bool): whether to add virtual node or not\n",
    "        '''\n",
    "\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.num_layer = num_layer\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.JK = JK\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_class = num_class\n",
    "        self.graph_pooling = graph_pooling\n",
    "\n",
    "        if self.num_layer < 2:\n",
    "            raise ValueError(\"Number of GNN layers must be greater than 1.\")\n",
    "\n",
    "        ### GNN to generate node embeddings\n",
    "        if virtual_node:\n",
    "            self.gnn_node = GNN_node_Virtualnode(num_layer, emb_dim, JK = JK, drop_ratio = drop_ratio, residual = residual, gnn_type = gnn_type)\n",
    "        else:\n",
    "            self.gnn_node = GNN_node(num_layer, emb_dim, JK = JK, drop_ratio = drop_ratio, residual = residual, gnn_type = gnn_type)\n",
    "\n",
    "\n",
    "        ### Pooling function to generate whole-graph embeddings\n",
    "        if self.graph_pooling == \"sum\":\n",
    "            self.pool = global_add_pool\n",
    "        elif self.graph_pooling == \"mean\":\n",
    "            self.pool = global_mean_pool\n",
    "        elif self.graph_pooling == \"max\":\n",
    "            self.pool = global_max_pool\n",
    "        elif self.graph_pooling == \"attention\":\n",
    "            self.pool = GlobalAttention(gate_nn = torch.nn.Sequential(torch.nn.Linear(emb_dim, 2*emb_dim), torch.nn.BatchNorm1d(2*emb_dim), torch.nn.ReLU(), torch.nn.Linear(2*emb_dim, 1)))\n",
    "        elif self.graph_pooling == \"set2set\":\n",
    "            self.pool = Set2Set(emb_dim, processing_steps = 2)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid graph pooling type.\")\n",
    "\n",
    "        if graph_pooling == \"set2set\":\n",
    "            self.graph_pred_linear = torch.nn.Linear(2*self.emb_dim, self.num_class)\n",
    "        else:\n",
    "            self.graph_pred_linear = torch.nn.Linear(self.emb_dim, self.num_class)\n",
    "\n",
    "    def forward(self, batched_data, return_embeddings=False):\n",
    "        h_node = self.gnn_node(batched_data)\n",
    "        h_graph = self.pool(h_node, batched_data.batch)  # les embeddings de graphes\n",
    "        logits = self.graph_pred_linear(h_graph)\n",
    "\n",
    "        if return_embeddings:\n",
    "            return logits, F.normalize(h_graph, p=2, dim=1)  # on retourne aussi les embeddings normalisés si demandé\n",
    "        else:\n",
    "            return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a9c70d7",
   "metadata": {
    "id": "Dyf0I2-t9IcW",
    "papermill": {
     "duration": 0.019268,
     "end_time": "2025-05-21T16:25:02.544583",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.525315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_zeros(data):\n",
    "    data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6139b912",
   "metadata": {
    "id": "8peFiIS19ZpK",
    "papermill": {
     "duration": 0.017908,
     "end_time": "2025-05-21T16:25:02.607848",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.589940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(data_loader, model, device, calculate_accuracy=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    total_loss = 0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader, desc=\"Iterating eval graphs\", unit=\"batch\"):\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            \n",
    "            if calculate_accuracy:\n",
    "                correct += (pred == data.y).sum().item()\n",
    "                total += data.y.size(0)\n",
    "                total_loss += criterion(output, data.y).item()\n",
    "            else:\n",
    "                predictions.extend(pred.cpu().numpy())\n",
    "    if calculate_accuracy:\n",
    "        accuracy = correct / total\n",
    "        return  total_loss / len(data_loader),accuracy\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbdbd871",
   "metadata": {
    "id": "WanuZKxy9Zs-",
    "papermill": {
     "duration": 0.016728,
     "end_time": "2025-05-21T16:25:02.635694",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.618966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_predictions(predictions, test_path):\n",
    "    script_dir = os.getcwd() \n",
    "    submission_folder = os.path.join(script_dir, \"submission\")\n",
    "    test_dir_name = os.path.basename(os.path.dirname(test_path))\n",
    "    \n",
    "    os.makedirs(submission_folder, exist_ok=True)\n",
    "    \n",
    "    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n",
    "    \n",
    "    test_graph_ids = list(range(len(predictions)))\n",
    "    output_df = pd.DataFrame({\n",
    "        \"id\": test_graph_ids,\n",
    "        \"pred\": predictions\n",
    "    })\n",
    "    \n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc3d24da",
   "metadata": {
    "id": "uyHIJS5U9ZzB",
    "papermill": {
     "duration": 0.017765,
     "end_time": "2025-05-21T16:25:02.664538",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.646773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_training_progress(train_losses, train_accuracies, output_dir):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Training Loss\", color='blue')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss per Epoch')\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color='green')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training Accuracy per Epoch')\n",
    "\n",
    "    # Save plots in the current directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"training_progress.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22574fa5",
   "metadata": {
    "papermill": {
     "duration": 0.016577,
     "end_time": "2025-05-21T16:25:02.692205",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.675628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_user_input(prompt, default=None, required=False, type_cast=str):\n",
    "\n",
    "    while True:\n",
    "        user_input = input(f\"{prompt} [{default}]: \")\n",
    "        \n",
    "        if user_input == \"\" and required:\n",
    "            print(\"This field is required. Please enter a value.\")\n",
    "            continue\n",
    "        \n",
    "        if user_input == \"\" and default is not None:\n",
    "            return default\n",
    "        \n",
    "        if user_input == \"\" and not required:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            return type_cast(user_input)\n",
    "        except ValueError:\n",
    "            print(f\"Invalid input. Please enter a valid {type_cast.__name__}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "139e88b2",
   "metadata": {
    "papermill": {
     "duration": 0.017703,
     "end_time": "2025-05-21T16:25:02.721184",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.703481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_arguments():\n",
    "    args = {}\n",
    "    args['train_path'] = get_user_input(\"Path to the training dataset (optional)\")\n",
    "    args['test_path'] = get_user_input(\"Path to the test dataset\", required=True)\n",
    "    args['num_checkpoints'] = get_user_input(\"Number of checkpoints to save during training\", type_cast=int)\n",
    "    args['device'] = get_user_input(\"Which GPU to use if any\", default=1, type_cast=int)\n",
    "    args['gnn'] = get_user_input(\"GNN type (gin, gin-virtual, gcn, gcn-virtual)\", default='gin')\n",
    "    args['drop_ratio'] = get_user_input(\"Dropout ratio\", default=0.0, type_cast=float)\n",
    "    args['num_layer'] = get_user_input(\"Number of GNN message passing layers\", default=5, type_cast=int)\n",
    "    args['emb_dim'] = get_user_input(\"Dimensionality of hidden units in GNNs\", default=300, type_cast=int)\n",
    "    args['batch_size'] = get_user_input(\"Input batch size for training\", default=32, type_cast=int)\n",
    "    args['epochs'] = get_user_input(\"Number of epochs to train\", default=10, type_cast=int)\n",
    "    args['baseline_mode'] = get_user_input(\"Baseline mode: 1 (CE), 2 (GCOD)\", default=1, type_cast=int)\n",
    "    args['noise_prob'] = get_user_input(\"Noise probability p (used if baseline_mode=2)\", default=0.2, type_cast=float)\n",
    "\n",
    "    \n",
    "    return argparse.Namespace(**args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45bffa19",
   "metadata": {
    "papermill": {
     "duration": 0.118164,
     "end_time": "2025-05-21T16:25:02.850799",
     "exception": true,
     "start_time": "2025-05-21T16:25:02.732635",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Path to the training dataset (optional) [None]:  datasets/A/train.json.gz\n",
      "Path to the test dataset [None]:  datasets/A/test.json.gz\n",
      "Number of checkpoints to save during training [None]:  20\n",
      "Which GPU to use if any [1]:  \n",
      "GNN type (gin, gin-virtual, gcn, gcn-virtual) [gin]:  gcn\n",
      "Dropout ratio [0.0]:  0.5\n",
      "Number of GNN message passing layers [5]:  \n",
      "Dimensionality of hidden units in GNNs [300]:  \n",
      "Input batch size for training [32]:  \n",
      "Number of epochs to train [10]:  600\n",
      "Baseline mode: 1 (CE), 2 (GCOD) [1]:  2\n",
      "Noise probability p (used if baseline_mode=2) [0.2]:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments received:\n",
      "train_path: datasets/A/train.json.gz\n",
      "test_path: datasets/A/test.json.gz\n",
      "num_checkpoints: 20\n",
      "device: 1\n",
      "gnn: gcn\n",
      "drop_ratio: 0.5\n",
      "num_layer: 5\n",
      "emb_dim: 300\n",
      "batch_size: 32\n",
      "epochs: 600\n",
      "baseline_mode: 2\n",
      "noise_prob: 0.2\n"
     ]
    }
   ],
   "source": [
    "def populate_args(args):\n",
    "    print(\"Arguments received:\")\n",
    "    for key, value in vars(args).items():\n",
    "        print(f\"{key}: {value}\")\n",
    "args = get_arguments()\n",
    "populate_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f58745df",
   "metadata": {
    "id": "lHX55XGECXBr",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "script_dir = os.getcwd() \n",
    "# device = torch.device(f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_checkpoints = args.num_checkpoints if args.num_checkpoints else 3\n",
    "    \n",
    "if args.gnn == 'gin':\n",
    "    model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False).to(device)\n",
    "elif args.gnn == 'gin-virtual':\n",
    "    model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True).to(device)\n",
    "elif args.gnn == 'gcn':\n",
    "    model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False).to(device)\n",
    "elif args.gnn == 'gcn-virtual':\n",
    "    model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True).to(device)\n",
    "else:\n",
    "    raise ValueError('Invalid GNN type')\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "if args.baseline_mode == 2:\n",
    "    # We initializa criterion and optimizers in the cell after the definition of full_dataset, as we need train_size\n",
    "    pass\n",
    "else:\n",
    "    criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a025914",
   "metadata": {
    "id": "BTYT5jYuChPb",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dir_name = os.path.basename(os.path.dirname(args.test_path))\n",
    "logs_folder = os.path.join(script_dir, \"logs\", test_dir_name)\n",
    "log_file = os.path.join(logs_folder, \"training.log\")\n",
    "os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logging.getLogger().addHandler(logging.StreamHandler())\n",
    "\n",
    "checkpoint_path = os.path.join(script_dir, \"checkpoints\", f\"model_{test_dir_name}_best.pth\")\n",
    "checkpoints_folder = os.path.join(script_dir, \"checkpoints\", test_dir_name)\n",
    "os.makedirs(checkpoints_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbca6779",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists(checkpoint_path) and not args.train_path:\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    print(f\"Loaded best model from {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40cbffaf-190a-4e81-8be1-9972a709ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a class to give indexes for the train set (subset of the full set (train and val))\n",
    "class IndexedSubDataset(Dataset):\n",
    "    def __init__(self, subset):\n",
    "        self.subset = subset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        data = self.subset[i]\n",
    "        data.idx = i\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96036e5b-e165-4bf7-9665-19cf3c17861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCODLoss(nn.Module):\n",
    "    #def __init__(self, num_samples, num_classes=6, alpha=1.0, beta=1.0):\n",
    "    def __init__(self, num_classes=6, alpha=1.0, beta=1.0):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "\n",
    "    def forward(self, logits, u_batch, y_onehot, y_soft, a_train):\n",
    "        \"\"\"\n",
    "        logits: (B, C)\n",
    "        u_batch: (B,)\n",
    "        y_onehot: (B, C)\n",
    "        y_soft: (B, C)\n",
    "        a_train: float\n",
    "        \"\"\"\n",
    "        B, C = logits.shape\n",
    "\n",
    "        u_diag_y = u_batch.unsqueeze(1) * y_onehot  # (B, C)\n",
    "\n",
    "        # L1: weighted cross-entropy with soft labels\n",
    "        logits_mod = logits + a_train * u_diag_y\n",
    "        L1 = F.cross_entropy(logits_mod, y_soft.argmax(dim=1))\n",
    "\n",
    "        # L2: squared norm alignment\n",
    "        y_pred_onehot = torch.zeros_like(y_onehot)\n",
    "        y_pred_onehot.scatter_(1, logits.argmax(dim=1, keepdim=True), 1)\n",
    "        L2 = (1 / C) * ((y_pred_onehot + u_diag_y - y_onehot).pow(2).sum(dim=1).mean())\n",
    "\n",
    "        # L3: KL regularization\n",
    "        # Get fθ(ZB) @ yB^T → logits of the true class for each sample\n",
    "        # logits: (B, C), y_onehot: (B, C)\n",
    "        eps = 1e-12\n",
    "        # Step 1: Matrix multiplication (B x C) @ (C x B) = (B x B)\n",
    "        logits_yT = torch.matmul(logits, y_onehot.T)  # (B, B)\n",
    "    \n",
    "        # Step 2: Extract diagonal\n",
    "        diag_logits_y = torch.diag(logits_yT)  # (B,)\n",
    "    \n",
    "        # Step 3: Softmax + log → log P\n",
    "        log_P = F.log_softmax(diag_logits_y, dim=0)\n",
    "        P = torch.exp(log_P)\n",
    "    \n",
    "        # Step 4: Softmax of -log(u)\n",
    "        log_u = torch.log(u_batch.clamp(min=eps))\n",
    "        Q = F.softmax(-log_u, dim=0)\n",
    "        log_Q = torch.log(Q + eps)\n",
    "    \n",
    "        # Step 5: KL(P || Q)\n",
    "        L3 = (1 - a_train) * torch.sum(P * (log_P - log_Q))\n",
    "\n",
    "\n",
    "        return L1 + L3, L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9acafb80-607f-4ee3-9a46-cc72479c47e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer_theta, optimizer_u, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        optimizer_theta.zero_grad()\n",
    "        optimizer_u.zero_grad()\n",
    "\n",
    "        logits, embeddings = model(batch, return_embeddings=True) \n",
    "        y = batch.y\n",
    "        y_onehot = F.one_hot(y, num_classes=criterion.num_classes).float()\n",
    "\n",
    "        # Compute y_soft \n",
    "        # See paper : Wani, F. A., Bucarelli, M. S., and Silvestri, F. (2023).\n",
    "        # Combining distance to class centroids and outlier discounting for improved learning with noisy labels \n",
    "        \n",
    "        # Compute embeddings centroïds for each class \n",
    "        class_means = {}\n",
    "        for c in torch.unique(y):\n",
    "            mask = (y == c)\n",
    "            class_embs = embeddings[mask]\n",
    "            if class_embs.size(0) > 0:\n",
    "                mean_emb = class_embs.mean(dim=0)\n",
    "                norm_mean_emb = mean_emb / (mean_emb.norm(p=2) + 1e-12)  # normalize\n",
    "                class_means[c.item()] = norm_mean_emb  # shape: [embedding_dim]\n",
    "        \n",
    "        # y_soft initialized to zero\n",
    "        y_soft = torch.zeros_like(y_onehot)  # shape: [batch_size, num_classes]\n",
    "        \n",
    "        for i in range(len(y)):\n",
    "            emb_i = embeddings[i]\n",
    "            norm_emb_i = emb_i / (emb_i.norm(p=2) + 1e-12)\n",
    "            class_idx = y[i].item()\n",
    "        \n",
    "            if class_idx in class_means:\n",
    "                sim = torch.dot(norm_emb_i, class_means[class_idx])\n",
    "                sim = torch.clamp(sim, min=0.0)  # ReLU\n",
    "                y_soft[i, class_idx] = sim\n",
    "            else:\n",
    "                y_soft[i, class_idx] = 0.0\n",
    "\n",
    "\n",
    "\n",
    "        # Get indices\n",
    "        graph_indices = batch.idx.to(device)\n",
    "        u_batch = u[graph_indices]  # 🔁 u est passé de l'extérieur\n",
    "\n",
    "        #  Clamp u between 0 and 1 (u is a confidence index for the level of noise of the label)\n",
    "        u.data.clamp_(0, 1)\n",
    "\n",
    "        # predictions for accuracy\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "        train_acc = correct / total # train_acc = a_train in the paper 'Robustness of Graph Classification ...'\n",
    "\n",
    "        # losses\n",
    "        loss_theta, loss_u = criterion(logits, u_batch, y_onehot, y_soft, train_acc)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss_theta.backward(retain_graph=True)\n",
    "        optimizer_theta.step()\n",
    "\n",
    "        loss_u.backward()\n",
    "        optimizer_u.step()\n",
    "\n",
    "        #  Clamp u between 0 and 1 (u is a confidence index for the level of noise of the label)\n",
    "        u.data.clamp_(0, 1)\n",
    "\n",
    "\n",
    "        total_loss += loss_theta.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    final_train_acc = correct / total\n",
    "    return avg_loss, final_train_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64e363d9-4b20-49ee-bd97-44e9e5608b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading graphs from datasets/A/train.json.gz...\n",
      "This may take a few minutes, please wait...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing graphs: 100%|██████████| 11280/11280 [01:02<00:00, 180.37graph/s]\n",
      "Iterating eval graphs: 100%|██████████| 71/71 [00:04<00:00, 14.40batch/s]\n",
      "Epoch 1/600, Loss: 1.6689, Train Acc: 0.3546, Val Acc: 0.4145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600, Loss: 1.6689, Train Acc: 0.3546, Val Acc: 0.4145\n",
      "Best model updated and saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/model_A_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 71/71 [00:04<00:00, 14.42batch/s]\n",
      "Epoch 2/600, Loss: 5.0540, Train Acc: 0.3733, Val Acc: 0.4317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/600, Loss: 5.0540, Train Acc: 0.3733, Val Acc: 0.4317\n",
      "Best model updated and saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/model_A_best.pth\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m     checkpoint_intervals = [num_epochs]\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     train_loss, train_acc = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_theta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_u\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m     val_loss, val_acc = evaluate(val_loader, model, device, calculate_accuracy=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     56\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(train_loader, model, optimizer_theta, optimizer_u, criterion, device)\u001b[39m\n\u001b[32m     10\u001b[39m optimizer_theta.zero_grad()\n\u001b[32m     11\u001b[39m optimizer_u.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m logits, embeddings = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \n\u001b[32m     14\u001b[39m y = batch.y\n\u001b[32m     15\u001b[39m y_onehot = F.one_hot(y, num_classes=criterion.num_classes).float()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mGNN.forward\u001b[39m\u001b[34m(self, batched_data, return_embeddings)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, batched_data, return_embeddings=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     h_node = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgnn_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     h_graph = \u001b[38;5;28mself\u001b[39m.pool(h_node, batched_data.batch)  \u001b[38;5;66;03m# les embeddings de graphes\u001b[39;00m\n\u001b[32m     61\u001b[39m     logits = \u001b[38;5;28mself\u001b[39m.graph_pred_linear(h_graph)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/DL-Hackathon/hackaton/src/conv.py:114\u001b[39m, in \u001b[36mGNN_node.forward\u001b[39m\u001b[34m(self, batched_data)\u001b[39m\n\u001b[32m    111\u001b[39m h_list = [\u001b[38;5;28mself\u001b[39m.node_encoder(x)]\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_layer):\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     h = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     h = \u001b[38;5;28mself\u001b[39m.batch_norms[layer](h)\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m layer == \u001b[38;5;28mself\u001b[39m.num_layer - \u001b[32m1\u001b[39m:\n\u001b[32m    118\u001b[39m         \u001b[38;5;66;03m#remove relu for the last layer\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/DL-Hackathon/hackaton/src/conv.py:57\u001b[39m, in \u001b[36mGCNConv.forward\u001b[39m\u001b[34m(self, x, edge_index, edge_attr)\u001b[39m\n\u001b[32m     53\u001b[39m deg_inv_sqrt[deg_inv_sqrt == \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33minf\u001b[39m\u001b[33m'\u001b[39m)] = \u001b[32m0\u001b[39m\n\u001b[32m     55\u001b[39m norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m)\u001b[49m + F.relu(x + \u001b[38;5;28mself\u001b[39m.root_emb.weight) * \u001b[32m1.\u001b[39m/deg.view(-\u001b[32m1\u001b[39m,\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/src.conv_GCNConv_propagate_mmcl26fi.py:192\u001b[39m, in \u001b[36mpropagate\u001b[39m\u001b[34m(self, edge_index, x, edge_attr, norm, size)\u001b[39m\n\u001b[32m    182\u001b[39m             kwargs = CollectArgs(\n\u001b[32m    183\u001b[39m                 x_j=hook_kwargs[\u001b[33m'\u001b[39m\u001b[33mx_j\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    184\u001b[39m                 edge_attr=hook_kwargs[\u001b[33m'\u001b[39m\u001b[33medge_attr\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    188\u001b[39m                 dim_size=kwargs.dim_size,\n\u001b[32m    189\u001b[39m             )\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# End Message Forward Pre Hook #########################################\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx_j\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mx_j\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# Begin Message Forward Hook ###########################################\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/DL-Hackathon/hackaton/src/conv.py:60\u001b[39m, in \u001b[36mGCNConv.message\u001b[39m\u001b[34m(self, x_j, edge_attr, norm)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmessage\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_j, edge_attr, norm):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m norm.view(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m) * \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_j\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/functional.py:1693\u001b[39m, in \u001b[36mrelu\u001b[39m\u001b[34m(input, inplace)\u001b[39m\n\u001b[32m   1681\u001b[39m threshold = _threshold\n\u001b[32m   1683\u001b[39m threshold_ = _add_docstr(\n\u001b[32m   1684\u001b[39m     _VF.threshold_,\n\u001b[32m   1685\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1689\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m,\n\u001b[32m   1690\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1693\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrelu\u001b[39m(\u001b[38;5;28minput\u001b[39m: Tensor, inplace: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> Tensor:  \u001b[38;5;66;03m# noqa: D400,D402\u001b[39;00m\n\u001b[32m   1694\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"relu(input, inplace=False) -> Tensor\u001b[39;00m\n\u001b[32m   1695\u001b[39m \n\u001b[32m   1696\u001b[39m \u001b[33;03m    Applies the rectified linear unit function element-wise. See\u001b[39;00m\n\u001b[32m   1697\u001b[39m \u001b[33;03m    :class:`~torch.nn.ReLU` for more details.\u001b[39;00m\n\u001b[32m   1698\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1699\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "if args.train_path:\n",
    "    full_dataset = GraphDataset(args.train_path, transform=add_zeros)\n",
    "    val_size = int(0.2 * len(full_dataset))\n",
    "    train_size = len(full_dataset) - val_size\n",
    "\n",
    "    generator = torch.Generator().manual_seed(12)\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "    train_dataset = IndexedSubDataset(train_dataset)\n",
    "\n",
    "    if args.baseline_mode == 2:\n",
    "\n",
    "        u = nn.Parameter(torch.zeros(len(train_dataset), device=device))\n",
    "        optimizer_u = torch.optim.Adam([u], lr=1)\n",
    "        optimizer_theta = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "        #scheduler = ReduceLROnPlateau(\n",
    "        #optimizer_theta, \n",
    "        #mode='max',           # maximiser val_acc\n",
    "        #factor=0.5,           # réduction du LR par 2\n",
    "        #patience=5,           # n epochs sans amélioration\n",
    "        #threshold=1e-3,       # tolérance d’amélioration\n",
    "        #min_lr=1e-6           # LR minimum\n",
    "        #)\n",
    "\n",
    "\n",
    "        \n",
    "        criterion = GCODLoss(num_classes=6, alpha=1.0, beta=1.0)\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False)  # shuffle=False mandatory, not to loose indexes ?\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    num_epochs = args.epochs\n",
    "    best_val_accuracy = 0.0\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    if num_checkpoints > 1:\n",
    "        checkpoint_intervals = [int((i + 1) * num_epochs / num_checkpoints) for i in range(num_checkpoints)]\n",
    "    else:\n",
    "        checkpoint_intervals = [num_epochs]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train(\n",
    "            train_loader, model, optimizer_theta, optimizer_u, criterion, device\n",
    "        )\n",
    "\n",
    "        val_loss, val_acc = evaluate(val_loader, model, device, calculate_accuracy=True)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        logging.info(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        # 🔽 Appel du scheduler avec la métrique à surveiller\n",
    "        #scheduler.step(val_acc)\n",
    "\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Best model updated and saved at {checkpoint_path}\")\n",
    "\n",
    "    plot_training_progress(train_losses, train_accuracies, os.path.join(logs_folder, \"plots\"))\n",
    "    plot_training_progress(val_losses, val_accuracies, os.path.join(logs_folder, \"plotsVal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07594aff",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "del train_dataset\n",
    "del train_loader\n",
    "del full_dataset\n",
    "del val_dataset\n",
    "del val_loader\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba668fff",
   "metadata": {
    "id": "xsXZIj4Mdu3I",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = GraphDataset(args.test_path, transform=add_zeros)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7828e983",
   "metadata": {
    "id": "x1OnGq_nCmTr",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "predictions = evaluate(test_loader, model, device, calculate_accuracy=False)\n",
    "save_predictions(predictions, args.test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729eea6c-f238-4c5b-a8e4-9aa0cbfd2d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c852a501-a976-46bb-8a27-08bddf896ca6",
   "metadata": {},
   "source": [
    "# B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75b1990-855b-4b01-b35f-1bae2ef504a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.train_path = 'datasets/B/train.json.gz'\n",
    "args.test_path = 'datasets/B/test.json.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99cc63c-a659-4bca-b588-6dbf547a7581",
   "metadata": {
    "id": "lHX55XGECXBr",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "script_dir = os.getcwd() \n",
    "# device = torch.device(f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_checkpoints = args.num_checkpoints if args.num_checkpoints else 3\n",
    "    \n",
    "if args.gnn == 'gin':\n",
    "    model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False).to(device)\n",
    "elif args.gnn == 'gin-virtual':\n",
    "    model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True).to(device)\n",
    "elif args.gnn == 'gcn':\n",
    "    model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False).to(device)\n",
    "elif args.gnn == 'gcn-virtual':\n",
    "    model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True).to(device)\n",
    "else:\n",
    "    raise ValueError('Invalid GNN type')\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "if args.baseline_mode == 2:\n",
    "    # We initializa criterion and optimizers in the cell after the definition of full_dataset, as we need train_size\n",
    "    pass\n",
    "else:\n",
    "    criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9536867-c85f-4583-b753-ddd92ef62bc3",
   "metadata": {
    "id": "BTYT5jYuChPb",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dir_name = os.path.basename(os.path.dirname(args.test_path))\n",
    "logs_folder = os.path.join(script_dir, \"logs\", test_dir_name)\n",
    "log_file = os.path.join(logs_folder, \"training.log\")\n",
    "os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logging.getLogger().addHandler(logging.StreamHandler())\n",
    "\n",
    "checkpoint_path = os.path.join(script_dir, \"checkpoints\", f\"model_{test_dir_name}_best.pth\")\n",
    "checkpoints_folder = os.path.join(script_dir, \"checkpoints\", test_dir_name)\n",
    "os.makedirs(checkpoints_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbc1c28-9391-4ce8-bc9e-3a55501b50f1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists(checkpoint_path) and not args.train_path:\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    print(f\"Loaded best model from {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104cc5ea-147b-42c7-a4e8-92c4b4a41789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "if args.train_path:\n",
    "    full_dataset = GraphDataset(args.train_path, transform=add_zeros)\n",
    "    val_size = int(0.2 * len(full_dataset))\n",
    "    train_size = len(full_dataset) - val_size\n",
    "\n",
    "    generator = torch.Generator().manual_seed(12)\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "    train_dataset = IndexedSubDataset(train_dataset)\n",
    "\n",
    "    if args.baseline_mode == 2:\n",
    "\n",
    "        u = nn.Parameter(torch.zeros(len(train_dataset), device=device))\n",
    "        optimizer_u = torch.optim.Adam([u], lr=1)\n",
    "        optimizer_theta = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "        #scheduler = ReduceLROnPlateau(\n",
    "        #optimizer_theta, \n",
    "        #mode='max',           # maximiser val_acc\n",
    "        #factor=0.5,           # réduction du LR par 2\n",
    "        #patience=5,           # n epochs sans amélioration\n",
    "        #threshold=1e-3,       # tolérance d’amélioration\n",
    "        #min_lr=1e-6           # LR minimum\n",
    "        #)\n",
    "\n",
    "\n",
    "        \n",
    "        criterion = GCODLoss(num_classes=6, alpha=1.0, beta=1.0)\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False)  # shuffle=False mandatory, not to loose indexes ?\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    num_epochs = args.epochs\n",
    "    best_val_accuracy = 0.0\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    if num_checkpoints > 1:\n",
    "        checkpoint_intervals = [int((i + 1) * num_epochs / num_checkpoints) for i in range(num_checkpoints)]\n",
    "    else:\n",
    "        checkpoint_intervals = [num_epochs]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train(\n",
    "            train_loader, model, optimizer_theta, optimizer_u, criterion, device\n",
    "        )\n",
    "\n",
    "        val_loss, val_acc = evaluate(val_loader, model, device, calculate_accuracy=True)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        logging.info(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        # 🔽 Appel du scheduler avec la métrique à surveiller\n",
    "        #scheduler.step(val_acc)\n",
    "\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Best model updated and saved at {checkpoint_path}\")\n",
    "\n",
    "    plot_training_progress(train_losses, train_accuracies, os.path.join(logs_folder, \"plots\"))\n",
    "    plot_training_progress(val_losses, val_accuracies, os.path.join(logs_folder, \"plotsVal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b158ad-caef-45e4-a9c6-1cc51db625c1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "del train_dataset\n",
    "del train_loader\n",
    "del full_dataset\n",
    "del val_dataset\n",
    "del val_loader\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893f449f-d056-4e40-81ef-ba47c3e51613",
   "metadata": {
    "id": "xsXZIj4Mdu3I",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = GraphDataset(args.test_path, transform=add_zeros)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2172aa4a-f620-4ccf-8d32-6ca93892c871",
   "metadata": {
    "id": "x1OnGq_nCmTr",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "predictions = evaluate(test_loader, model, device, calculate_accuracy=False)\n",
    "save_predictions(predictions, args.test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e42a0-38af-4fff-9985-6355a5acdc2a",
   "metadata": {},
   "source": [
    "# C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1213f0fa-8916-46a9-9933-30ef0b647a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.train_path = 'datasets/C/train.json.gz'\n",
    "args.test_path = 'datasets/C/test.json.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57982647-66ca-42cc-bef2-8e5fe105ff41",
   "metadata": {
    "id": "lHX55XGECXBr",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "script_dir = os.getcwd() \n",
    "# device = torch.device(f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_checkpoints = args.num_checkpoints if args.num_checkpoints else 3\n",
    "    \n",
    "if args.gnn == 'gin':\n",
    "    model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False).to(device)\n",
    "elif args.gnn == 'gin-virtual':\n",
    "    model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True).to(device)\n",
    "elif args.gnn == 'gcn':\n",
    "    model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False).to(device)\n",
    "elif args.gnn == 'gcn-virtual':\n",
    "    model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True).to(device)\n",
    "else:\n",
    "    raise ValueError('Invalid GNN type')\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "if args.baseline_mode == 2:\n",
    "    # We initializa criterion and optimizers in the cell after the definition of full_dataset, as we need train_size\n",
    "    pass\n",
    "else:\n",
    "    criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15150e24-98a0-4ebb-abfc-f0a429b9396a",
   "metadata": {
    "id": "BTYT5jYuChPb",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dir_name = os.path.basename(os.path.dirname(args.test_path))\n",
    "logs_folder = os.path.join(script_dir, \"logs\", test_dir_name)\n",
    "log_file = os.path.join(logs_folder, \"training.log\")\n",
    "os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logging.getLogger().addHandler(logging.StreamHandler())\n",
    "\n",
    "checkpoint_path = os.path.join(script_dir, \"checkpoints\", f\"model_{test_dir_name}_best.pth\")\n",
    "checkpoints_folder = os.path.join(script_dir, \"checkpoints\", test_dir_name)\n",
    "os.makedirs(checkpoints_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbcb569-7704-4913-84d3-ed52a30610e0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists(checkpoint_path) and not args.train_path:\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    print(f\"Loaded best model from {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f920bd-e8de-4326-8198-0cc94c95ecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "if args.train_path:\n",
    "    full_dataset = GraphDataset(args.train_path, transform=add_zeros)\n",
    "    val_size = int(0.2 * len(full_dataset))\n",
    "    train_size = len(full_dataset) - val_size\n",
    "\n",
    "    generator = torch.Generator().manual_seed(12)\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "    train_dataset = IndexedSubDataset(train_dataset)\n",
    "\n",
    "    if args.baseline_mode == 2:\n",
    "\n",
    "        u = nn.Parameter(torch.zeros(len(train_dataset), device=device))\n",
    "        optimizer_u = torch.optim.Adam([u], lr=1)\n",
    "        optimizer_theta = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "        #scheduler = ReduceLROnPlateau(\n",
    "        #optimizer_theta, \n",
    "        #mode='max',           # maximiser val_acc\n",
    "        #factor=0.5,           # réduction du LR par 2\n",
    "        #patience=5,           # n epochs sans amélioration\n",
    "        #threshold=1e-3,       # tolérance d’amélioration\n",
    "        #min_lr=1e-6           # LR minimum\n",
    "        #)\n",
    "\n",
    "\n",
    "        \n",
    "        criterion = GCODLoss(num_classes=6, alpha=1.0, beta=1.0)\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False)  # shuffle=False mandatory, not to loose indexes ?\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    num_epochs = args.epochs\n",
    "    best_val_accuracy = 0.0\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    if num_checkpoints > 1:\n",
    "        checkpoint_intervals = [int((i + 1) * num_epochs / num_checkpoints) for i in range(num_checkpoints)]\n",
    "    else:\n",
    "        checkpoint_intervals = [num_epochs]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train(\n",
    "            train_loader, model, optimizer_theta, optimizer_u, criterion, device\n",
    "        )\n",
    "\n",
    "        val_loss, val_acc = evaluate(val_loader, model, device, calculate_accuracy=True)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        logging.info(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        # 🔽 Appel du scheduler avec la métrique à surveiller\n",
    "        #scheduler.step(val_acc)\n",
    "\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Best model updated and saved at {checkpoint_path}\")\n",
    "\n",
    "    plot_training_progress(train_losses, train_accuracies, os.path.join(logs_folder, \"plots\"))\n",
    "    plot_training_progress(val_losses, val_accuracies, os.path.join(logs_folder, \"plotsVal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc89759-5430-4e7c-a05b-6bdebc4ff81f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "del train_dataset\n",
    "del train_loader\n",
    "del full_dataset\n",
    "del val_dataset\n",
    "del val_loader\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eabe043-2115-44b5-8459-b503a6067f1e",
   "metadata": {
    "id": "xsXZIj4Mdu3I",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = GraphDataset(args.test_path, transform=add_zeros)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7365abc9-7908-4354-ab92-98c912efb78a",
   "metadata": {
    "id": "x1OnGq_nCmTr",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "predictions = evaluate(test_loader, model, device, calculate_accuracy=False)\n",
    "save_predictions(predictions, args.test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e3c687-ff76-4993-b449-a06ed8c9938e",
   "metadata": {},
   "source": [
    "# D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c595f4-5aa9-49ae-8bb5-f2c959c64be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.train_path = 'datasets/D/train.json.gz'\n",
    "args.test_path = 'datasets/D/test.json.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1834c28e-70fe-4d6e-b66c-a69d494382c5",
   "metadata": {
    "id": "lHX55XGECXBr",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "script_dir = os.getcwd() \n",
    "# device = torch.device(f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_checkpoints = args.num_checkpoints if args.num_checkpoints else 3\n",
    "    \n",
    "if args.gnn == 'gin':\n",
    "    model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False).to(device)\n",
    "elif args.gnn == 'gin-virtual':\n",
    "    model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True).to(device)\n",
    "elif args.gnn == 'gcn':\n",
    "    model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False).to(device)\n",
    "elif args.gnn == 'gcn-virtual':\n",
    "    model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True).to(device)\n",
    "else:\n",
    "    raise ValueError('Invalid GNN type')\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "if args.baseline_mode == 2:\n",
    "    # We initializa criterion and optimizers in the cell after the definition of full_dataset, as we need train_size\n",
    "    pass\n",
    "else:\n",
    "    criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c38ec61-0ca3-447e-b7fe-b111ccac955c",
   "metadata": {
    "id": "BTYT5jYuChPb",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dir_name = os.path.basename(os.path.dirname(args.test_path))\n",
    "logs_folder = os.path.join(script_dir, \"logs\", test_dir_name)\n",
    "log_file = os.path.join(logs_folder, \"training.log\")\n",
    "os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logging.getLogger().addHandler(logging.StreamHandler())\n",
    "\n",
    "checkpoint_path = os.path.join(script_dir, \"checkpoints\", f\"model_{test_dir_name}_best.pth\")\n",
    "checkpoints_folder = os.path.join(script_dir, \"checkpoints\", test_dir_name)\n",
    "os.makedirs(checkpoints_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38de7a55-d52e-41a2-92a0-16497500d5c5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists(checkpoint_path) and not args.train_path:\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    print(f\"Loaded best model from {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d57d4fb-2c24-4082-a962-1e9a04afe1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "if args.train_path:\n",
    "    full_dataset = GraphDataset(args.train_path, transform=add_zeros)\n",
    "    val_size = int(0.2 * len(full_dataset))\n",
    "    train_size = len(full_dataset) - val_size\n",
    "\n",
    "    generator = torch.Generator().manual_seed(12)\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "    train_dataset = IndexedSubDataset(train_dataset)\n",
    "\n",
    "    if args.baseline_mode == 2:\n",
    "\n",
    "        u = nn.Parameter(torch.zeros(len(train_dataset), device=device))\n",
    "        optimizer_u = torch.optim.Adam([u], lr=1)\n",
    "        optimizer_theta = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "        #scheduler = ReduceLROnPlateau(\n",
    "        #optimizer_theta, \n",
    "        #mode='max',           # maximiser val_acc\n",
    "        #factor=0.5,           # réduction du LR par 2\n",
    "        #patience=5,           # n epochs sans amélioration\n",
    "        #threshold=1e-3,       # tolérance d’amélioration\n",
    "        #min_lr=1e-6           # LR minimum\n",
    "        #)\n",
    "\n",
    "\n",
    "        \n",
    "        criterion = GCODLoss(num_classes=6, alpha=1.0, beta=1.0)\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False)  # shuffle=False mandatory, not to loose indexes ?\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    num_epochs = args.epochs\n",
    "    best_val_accuracy = 0.0\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    if num_checkpoints > 1:\n",
    "        checkpoint_intervals = [int((i + 1) * num_epochs / num_checkpoints) for i in range(num_checkpoints)]\n",
    "    else:\n",
    "        checkpoint_intervals = [num_epochs]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train(\n",
    "            train_loader, model, optimizer_theta, optimizer_u, criterion, device\n",
    "        )\n",
    "\n",
    "        val_loss, val_acc = evaluate(val_loader, model, device, calculate_accuracy=True)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        logging.info(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        # 🔽 Appel du scheduler avec la métrique à surveiller\n",
    "        #scheduler.step(val_acc)\n",
    "\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Best model updated and saved at {checkpoint_path}\")\n",
    "\n",
    "    plot_training_progress(train_losses, train_accuracies, os.path.join(logs_folder, \"plots\"))\n",
    "    plot_training_progress(val_losses, val_accuracies, os.path.join(logs_folder, \"plotsVal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73b2490-298c-48db-a841-4129da012472",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "del train_dataset\n",
    "del train_loader\n",
    "del full_dataset\n",
    "del val_dataset\n",
    "del val_loader\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4d9100-9af4-499c-a254-649a4d808ca1",
   "metadata": {
    "id": "xsXZIj4Mdu3I",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = GraphDataset(args.test_path, transform=add_zeros)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9457899-93b7-45e8-997b-a440ac932c7e",
   "metadata": {
    "id": "x1OnGq_nCmTr",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "predictions = evaluate(test_loader, model, device, calculate_accuracy=False)\n",
    "save_predictions(predictions, args.test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ad9bfb-ae6d-48a4-85ac-1db7f9d89310",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1594ef-0f72-4716-b53f-8da074eae4d5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "def gzip_folder(folder_path, output_file):\n",
    "    \"\"\"\n",
    "    Compresses an entire folder into a single .tar.gz file.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder to compress.\n",
    "        output_file (str): Path to the output .gz file.\n",
    "    \"\"\"\n",
    "    with tarfile.open(output_file, \"w:gz\") as tar:\n",
    "        tar.add(folder_path, arcname=os.path.basename(folder_path))\n",
    "    print(f\"Folder '{folder_path}' has been compressed into '{output_file}'\")\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"/home/onyxia/work/DL-Hackathon/hackaton/submission\"            # Path to the folder you want to compress\n",
    "output_file = \"/home/onyxia/work/DL-Hackathon/hackaton/submission.gz\"        # Output .gz file name\n",
    "gzip_folder(folder_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1819921-968f-4457-ad59-3c960d92e8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "folder_path = \"/home/onyxia/work/DL-Hackathon/hackaton/logs\"            # Path to the folder you want to compress\n",
    "output_file = \"/home/onyxia/work/DL-Hackathon/hackaton/logs.gz\"        # Output .gz file name\n",
    "gzip_folder(folder_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f939db-06f0-4f43-92de-03b22cbb90c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "folder_path = \"/home/onyxia/work/DL-Hackathon/hackaton/checkpoints\"            # Path to the folder you want to compress\n",
    "output_file = \"/home/onyxia/work/DL-Hackathon/hackaton/checkpoints.gz\"        # Output .gz file name\n",
    "gzip_folder(folder_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd93cef-6ebd-4379-94d1-c1c386c40660",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 108.457758,
   "end_time": "2025-05-21T16:25:04.482169",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-21T16:23:16.024411",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
