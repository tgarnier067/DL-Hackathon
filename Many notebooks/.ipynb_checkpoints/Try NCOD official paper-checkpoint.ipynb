{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6228bc1c",
   "metadata": {
    "id": "xSkgt1zf-raF",
    "outputId": "59f4a52f-5eb4-41e5-9fba-07432989fe78",
    "papermill": {
     "duration": 5.620104,
     "end_time": "2025-05-21T16:23:24.361690",
     "exception": false,
     "start_time": "2025-05-21T16:23:18.741586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch_geometric torch gdown matplotlib --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86aceda",
   "metadata": {
    "id": "5oR2D2Us-xSQ",
    "outputId": "7086cadf-a7fe-4d75-f271-6339bee8164d",
    "papermill": {
     "duration": 4.049235,
     "end_time": "2025-05-21T16:23:28.415556",
     "exception": false,
     "start_time": "2025-05-21T16:23:24.366321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone --branch baselineCe https://github.com/Graph-Classification-Noisy-Label/hackaton.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c129091c",
   "metadata": {
    "id": "tEhfPly6-7UK",
    "outputId": "3078ee06-6312-4fca-f5f9-888fa628c80a",
    "papermill": {
     "duration": 0.013251,
     "end_time": "2025-05-21T16:23:28.434119",
     "exception": false,
     "start_time": "2025-05-21T16:23:28.420868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/onyxia/work/DL-Hackathon/hackaton\n"
     ]
    }
   ],
   "source": [
    "%cd hackaton/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d48103c0",
   "metadata": {
    "id": "PxBvwB0_6xI8",
    "outputId": "5933387c-2cfb-474f-d842-f36a3e2d2a73",
    "papermill": {
     "duration": 83.957666,
     "end_time": "2025-05-21T16:24:52.396720",
     "exception": false,
     "start_time": "2025-05-21T16:23:28.439054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents\n",
      "Retrieving folder 1wcUVBNQkZ04zStXkglXSgERfIvjSHJiL A\n",
      "Processing file 1C8sjkO6JS0j2SyVwQ07m8PhQ-pHpuI78 test.json.gz\n",
      "Processing file 12N11n8gufNA_C1ns-1IeBseBHgrSfRI1 train.json.gz\n",
      "Retrieving folder 1Tj5YoYYDDXjDxxi-cywZgoDkT0b1Qbz- B\n",
      "Processing file 11GBlrXMdP3HSD60w-56Tu6rbGkR-Ifww test.json.gz\n",
      "Processing file 13vp-Kwef3UgAwMG-dokGwKyARym9iqtL train.json.gz\n",
      "Retrieving folder 1e3B_tBMd693Iwv8x3zRR9c47l5yt_5ey C\n",
      "Processing file 18XVe65ZsQ0PDLCqQa4WmneVhyfjGcXmT test.json.gz\n",
      "Processing file 1z5lvG2CytbLQZt7Jmo9BopzFd0pKejEj train.json.gz\n",
      "Retrieving folder 1cvM0eZwpD4gzjo44_zdodxudVBMrLza1 D\n",
      "Processing file 1Gna_dHnBLX8vKaYGAAqAbw5QPerrNK1u test.json.gz\n",
      "Processing file 1Pc-6LMML80-AgEoLVs2Q5hLtmR_rTEek train.json.gz\n",
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1C8sjkO6JS0j2SyVwQ07m8PhQ-pHpuI78\n",
      "From (redirected): https://drive.google.com/uc?id=1C8sjkO6JS0j2SyVwQ07m8PhQ-pHpuI78&confirm=t&uuid=aedefd3b-7d12-44e4-8ae9-a908d07761f0\n",
      "To: /home/onyxia/work/DL-Hackathon/hackaton/datasets/A/test.json.gz\n",
      "100%|███████████████████████████████████████| 92.4M/92.4M [00:00<00:00, 105MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=12N11n8gufNA_C1ns-1IeBseBHgrSfRI1\n",
      "From (redirected): https://drive.google.com/uc?id=12N11n8gufNA_C1ns-1IeBseBHgrSfRI1&confirm=t&uuid=6506f94b-0871-4e45-af88-6d5eb1b42f20\n",
      "To: /home/onyxia/work/DL-Hackathon/hackaton/datasets/A/train.json.gz\n",
      "100%|████████████████████████████████████████| 465M/465M [00:08<00:00, 52.1MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=11GBlrXMdP3HSD60w-56Tu6rbGkR-Ifww\n",
      "From (redirected): https://drive.google.com/uc?id=11GBlrXMdP3HSD60w-56Tu6rbGkR-Ifww&confirm=t&uuid=4cc2ade0-6e0a-4aed-a4d6-2dff9a7fac00\n",
      "To: /home/onyxia/work/DL-Hackathon/hackaton/datasets/B/test.json.gz\n",
      "100%|██████████████████████████████████████| 63.0M/63.0M [00:01<00:00, 50.3MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=13vp-Kwef3UgAwMG-dokGwKyARym9iqtL\n",
      "From (redirected): https://drive.google.com/uc?id=13vp-Kwef3UgAwMG-dokGwKyARym9iqtL&confirm=t&uuid=8ab96d5e-deb0-4d7f-8d19-a41c5fe45c95\n",
      "To: /home/onyxia/work/DL-Hackathon/hackaton/datasets/B/train.json.gz\n",
      "100%|████████████████████████████████████████| 223M/223M [00:03<00:00, 68.0MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=18XVe65ZsQ0PDLCqQa4WmneVhyfjGcXmT\n",
      "From (redirected): https://drive.google.com/uc?id=18XVe65ZsQ0PDLCqQa4WmneVhyfjGcXmT&confirm=t&uuid=0fca3a0b-afde-4017-a258-6e867cd5e0cc\n",
      "To: /home/onyxia/work/DL-Hackathon/hackaton/datasets/C/test.json.gz\n",
      "100%|██████████████████████████████████████| 60.5M/60.5M [00:00<00:00, 89.6MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1z5lvG2CytbLQZt7Jmo9BopzFd0pKejEj\n",
      "From (redirected): https://drive.google.com/uc?id=1z5lvG2CytbLQZt7Jmo9BopzFd0pKejEj&confirm=t&uuid=19d850fe-03d7-4dd8-a8f1-cb2c42120a6d\n",
      "To: /home/onyxia/work/DL-Hackathon/hackaton/datasets/C/train.json.gz\n",
      "100%|█████████████████████████████████████████| 308M/308M [00:02<00:00, 107MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1Gna_dHnBLX8vKaYGAAqAbw5QPerrNK1u\n",
      "From (redirected): https://drive.google.com/uc?id=1Gna_dHnBLX8vKaYGAAqAbw5QPerrNK1u&confirm=t&uuid=7721fdb2-1576-4a13-918c-1f61bb6ae838\n",
      "To: /home/onyxia/work/DL-Hackathon/hackaton/datasets/D/test.json.gz\n",
      "100%|██████████████████████████████████████| 94.0M/94.0M [00:00<00:00, 99.3MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1Pc-6LMML80-AgEoLVs2Q5hLtmR_rTEek\n",
      "From (redirected): https://drive.google.com/uc?id=1Pc-6LMML80-AgEoLVs2Q5hLtmR_rTEek&confirm=t&uuid=27029279-ea78-4d33-a1ea-3a748006d8b0\n",
      "To: /home/onyxia/work/DL-Hackathon/hackaton/datasets/D/train.json.gz\n",
      "100%|████████████████████████████████████████| 439M/439M [00:04<00:00, 90.3MB/s]\n",
      "Download completed\n"
     ]
    }
   ],
   "source": [
    "!gdown --folder https://drive.google.com/drive/folders/1Z-1JkPJ6q4C6jX4brvq1VRbJH5RPUCAk -O datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80dab1bc",
   "metadata": {
    "id": "1rockhiQ7Nny",
    "outputId": "2cd2e6f4-5f8f-4a62-f0ec-53e6fc78c9b7",
    "papermill": {
     "duration": 0.138969,
     "end_time": "2025-05-21T16:24:52.549260",
     "exception": false,
     "start_time": "2025-05-21T16:24:52.410291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16K\n",
      "drwxrwsr-x 2 onyxia users 4.0K May 26 11:08 A\n",
      "drwxrwsr-x 2 onyxia users 4.0K May 26 11:09 B\n",
      "drwxrwsr-x 2 onyxia users 4.0K May 26 11:09 C\n",
      "drwxrwsr-x 2 onyxia users 4.0K May 26 11:09 D\n"
     ]
    }
   ],
   "source": [
    "!ls -lh datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "817b1078",
   "metadata": {
    "id": "lAQuCuIoBbq5",
    "papermill": {
     "duration": 9.949638,
     "end_time": "2025-05-21T16:25:02.510764",
     "exception": false,
     "start_time": "2025-05-21T16:24:52.561126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "# Load utility functions from cloned repository\n",
    "from src.loadData import GraphDataset\n",
    "from src.utils import set_seed\n",
    "#from src.models import GNN\n",
    "import argparse\n",
    "\n",
    "# On modifie la class GNN fourni par le prof, dans le dossier hackaton/src/models.py, afin d'extraire les embeddings, utiles pour NCOD\n",
    "\n",
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import global_add_pool, global_mean_pool, global_max_pool, GlobalAttention, Set2Set\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.inits import uniform\n",
    "\n",
    "from src.conv import GNN_node, GNN_node_Virtualnode\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_class, num_layer = 5, emb_dim = 300, \n",
    "                    gnn_type = 'gin', virtual_node = True, residual = False, drop_ratio = 0.5, JK = \"last\", graph_pooling = \"mean\"):\n",
    "        '''\n",
    "            num_tasks (int): number of labels to be predicted\n",
    "            virtual_node (bool): whether to add virtual node or not\n",
    "        '''\n",
    "\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.num_layer = num_layer\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.JK = JK\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_class = num_class\n",
    "        self.graph_pooling = graph_pooling\n",
    "\n",
    "        if self.num_layer < 2:\n",
    "            raise ValueError(\"Number of GNN layers must be greater than 1.\")\n",
    "\n",
    "        ### GNN to generate node embeddings\n",
    "        if virtual_node:\n",
    "            self.gnn_node = GNN_node_Virtualnode(num_layer, emb_dim, JK = JK, drop_ratio = drop_ratio, residual = residual, gnn_type = gnn_type)\n",
    "        else:\n",
    "            self.gnn_node = GNN_node(num_layer, emb_dim, JK = JK, drop_ratio = drop_ratio, residual = residual, gnn_type = gnn_type)\n",
    "\n",
    "\n",
    "        ### Pooling function to generate whole-graph embeddings\n",
    "        if self.graph_pooling == \"sum\":\n",
    "            self.pool = global_add_pool\n",
    "        elif self.graph_pooling == \"mean\":\n",
    "            self.pool = global_mean_pool\n",
    "        elif self.graph_pooling == \"max\":\n",
    "            self.pool = global_max_pool\n",
    "        elif self.graph_pooling == \"attention\":\n",
    "            self.pool = GlobalAttention(gate_nn = torch.nn.Sequential(torch.nn.Linear(emb_dim, 2*emb_dim), torch.nn.BatchNorm1d(2*emb_dim), torch.nn.ReLU(), torch.nn.Linear(2*emb_dim, 1)))\n",
    "        elif self.graph_pooling == \"set2set\":\n",
    "            self.pool = Set2Set(emb_dim, processing_steps = 2)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid graph pooling type.\")\n",
    "\n",
    "        if graph_pooling == \"set2set\":\n",
    "            self.graph_pred_linear = torch.nn.Linear(2*self.emb_dim, self.num_class)\n",
    "        else:\n",
    "            self.graph_pred_linear = torch.nn.Linear(self.emb_dim, self.num_class)\n",
    "\n",
    "#    def forward(self, batched_data):\n",
    "#        h_node = self.gnn_node(batched_data)\n",
    "\n",
    "#        h_graph = self.pool(h_node, batched_data.batch)\n",
    "\n",
    "#        return self.graph_pred_linear(h_graph)\n",
    "\n",
    "\n",
    "    def forward(self, batched_data):\n",
    "        h_node = self.gnn_node(batched_data)          # (N_nodes, emb_dim)\n",
    "        h_graph = self.pool(h_node, batched_data.batch)  # (B, emb_dim)\n",
    "\n",
    "        logits = self.graph_pred_linear(h_graph)      # (B, num_class)\n",
    "\n",
    "        return logits, h_graph  # <-- modification ici\n",
    "\n",
    "\n",
    "# Set the random seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "967c1b84-512b-4a01-ab49-5dd6888d0e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_zeros(data):\n",
    "    data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b943aff6-90dc-4d5e-b90c-f0edc93d282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader, model, device, calculate_accuracy=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    total_loss = 0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader, desc=\"Iterating eval graphs\", unit=\"batch\"):\n",
    "            data = data.to(device)\n",
    "#            output = model(data)[0] if isinstance(criterion, NCODCustomLoss) else model(data)\n",
    "#            pred = output.argmax(dim=1)\n",
    "\n",
    "            logits, _ = model(data)\n",
    "            pred = logits.argmax(dim=1)\n",
    "\n",
    "            \n",
    "            if calculate_accuracy:\n",
    "                correct += (pred == data.y).sum().item()\n",
    "                total += data.y.size(0)\n",
    "                total_loss += criterion(logits, data.y).item()\n",
    "            else:\n",
    "                predictions.extend(pred.cpu().numpy())\n",
    "    if calculate_accuracy:\n",
    "        accuracy = correct / total\n",
    "        return  total_loss / len(data_loader),accuracy\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fad4a41f-56a7-4b27-845d-1e4d5491c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(predictions, test_path):\n",
    "    script_dir = os.getcwd() \n",
    "    submission_folder = os.path.join(script_dir, \"submission\")\n",
    "    test_dir_name = os.path.basename(os.path.dirname(test_path))\n",
    "    \n",
    "    os.makedirs(submission_folder, exist_ok=True)\n",
    "    \n",
    "    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n",
    "    \n",
    "    test_graph_ids = list(range(len(predictions)))\n",
    "    output_df = pd.DataFrame({\n",
    "        \"id\": test_graph_ids,\n",
    "        \"pred\": predictions\n",
    "    })\n",
    "    \n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f77c6831-88a2-41f7-8b34-e945df2c73d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_progress(train_losses, train_accuracies, output_dir):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Training Loss\", color='blue')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss per Epoch')\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color='green')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training Accuracy per Epoch')\n",
    "\n",
    "    # Save plots in the current directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"training_progress.png\"))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7269ae1-b280-4946-a046-0594270032de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_input(prompt, default=None, required=False, type_cast=str):\n",
    "\n",
    "    while True:\n",
    "        user_input = input(f\"{prompt} [{default}]: \")\n",
    "        \n",
    "        if user_input == \"\" and required:\n",
    "            print(\"This field is required. Please enter a value.\")\n",
    "            continue\n",
    "        \n",
    "        if user_input == \"\" and default is not None:\n",
    "            return default\n",
    "        \n",
    "        if user_input == \"\" and not required:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            return type_cast(user_input)\n",
    "        except ValueError:\n",
    "            print(f\"Invalid input. Please enter a valid {type_cast.__name__}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f27cacb-4284-40e1-8d68-c059d864a45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arguments():\n",
    "    args = {}\n",
    "    args['train_path'] = get_user_input(\"Path to the training dataset (optional)\")\n",
    "    args['test_path'] = get_user_input(\"Path to the test dataset\", required=True)\n",
    "    args['num_checkpoints'] = get_user_input(\"Number of checkpoints to save during training\", type_cast=int)\n",
    "    args['device'] = get_user_input(\"Which GPU to use if any\", default=1, type_cast=int)\n",
    "    args['gnn'] = get_user_input(\"GNN type (gin, gin-virtual, gcn, gcn-virtual)\", default='gin')\n",
    "    args['drop_ratio'] = get_user_input(\"Dropout ratio\", default=0.0, type_cast=float)\n",
    "    args['num_layer'] = get_user_input(\"Number of GNN message passing layers\", default=5, type_cast=int)\n",
    "    args['emb_dim'] = get_user_input(\"Dimensionality of hidden units in GNNs\", default=300, type_cast=int)\n",
    "    args['batch_size'] = get_user_input(\"Input batch size for training\", default=32, type_cast=int)\n",
    "    args['epochs'] = get_user_input(\"Number of epochs to train\", default=10, type_cast=int)\n",
    "    args['baseline_mode'] = get_user_input(\"Baseline mode: 1 (CE), 2 (Noisy CE), 3 (NCOD)\", default=1, type_cast=int)\n",
    "    args['noise_prob'] = get_user_input(\"Noise probability p (used if baseline_mode=2)\", default=0.2, type_cast=float)\n",
    "\n",
    "    \n",
    "    return argparse.Namespace(**args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a030de5c-4c7d-4732-b9c3-98c3770b7f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments received:\n",
      "train_path: datasets/B/train.json.gz\n",
      "test_path: datasets/B/test.json.gz\n",
      "num_checkpoints: None\n",
      "device: 1\n",
      "gnn: gcn\n",
      "drop_ratio: 0.5\n",
      "num_layer: 5\n",
      "emb_dim: 300\n",
      "batch_size: 32\n",
      "epochs: 20\n",
      "baseline_mode: 3\n",
      "noise_prob: 0.2\n"
     ]
    }
   ],
   "source": [
    "def populate_args(args):\n",
    "    print(\"Arguments received:\")\n",
    "    for key, value in vars(args).items():\n",
    "        print(f\"{key}: {value}\")\n",
    "args = get_arguments()\n",
    "populate_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0caf99cd-431f-48f2-8c6c-e2d0bc1f96e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, p_noisy):\n",
    "        super().__init__()\n",
    "        self.p = p_noisy\n",
    "        self.ce = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        losses = self.ce(logits, targets)\n",
    "        weights = (1 - self.p) + self.p * (1 - torch.nn.functional.one_hot(targets, num_classes=logits.size(1)).float().sum(dim=1))\n",
    "        return (losses * weights).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a378bfb9-b20b-4733-b1b5-dd5597b9f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dir = os.getcwd() \n",
    "# device = torch.device(f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_checkpoints = args.num_checkpoints if args.num_checkpoints else 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89d87ff6-b915-4d6d-8608-c33988d623d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.gnn == 'gin':\n",
    "    model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False).to(device)\n",
    "elif args.gnn == 'gin-virtual':\n",
    "    model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True).to(device)\n",
    "elif args.gnn == 'gcn':\n",
    "    model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False).to(device)\n",
    "elif args.gnn == 'gcn-virtual':\n",
    "    model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True).to(device)\n",
    "else:\n",
    "    raise ValueError('Invalid GNN type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "981561af-df7c-4335-b222-fd3819f9bf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir_name = os.path.basename(os.path.dirname(args.test_path))\n",
    "logs_folder = os.path.join(script_dir, \"logs\", test_dir_name)\n",
    "log_file = os.path.join(logs_folder, \"training.log\")\n",
    "os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logging.getLogger().addHandler(logging.StreamHandler())\n",
    "\n",
    "checkpoint_path = os.path.join(script_dir, \"checkpoints\", f\"model_{test_dir_name}_best.pth\")\n",
    "checkpoints_folder = os.path.join(script_dir, \"checkpoints\", test_dir_name)\n",
    "os.makedirs(checkpoints_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdf2d23d-db20-44fc-9ecf-3f85f21d5299",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(checkpoint_path) and not args.train_path:\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    print(f\"Loaded best model from {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f84df7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NCOD de maria sofia\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "cross_entropy_val = nn.CrossEntropyLoss\n",
    "\n",
    "mean = 1e-8\n",
    "std = 1e-9\n",
    "encoder_features = args.emb_dim\n",
    "total_epochs = args.epochs\n",
    "\n",
    "\n",
    "class NCODLoss(nn.Module):\n",
    "    def __init__(self, sample_labels, num_examp=50000, num_classes=6, ratio_consistency=0, ratio_balance=0):\n",
    "        super(NCODLoss, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.USE_CUDA = torch.cuda.is_available()\n",
    "        self.num_examp = num_examp\n",
    "\n",
    "        self.ratio_consistency = ratio_consistency\n",
    "        self.ratio_balance = ratio_balance\n",
    "\n",
    "        self.u = nn.Parameter(torch.empty(num_examp, 1, dtype=torch.float32))\n",
    "        self.init_param(mean=mean, std=std)\n",
    "\n",
    "        self.beginning = True\n",
    "        self.prevSimilarity = torch.rand((num_examp, encoder_features))\n",
    "        self.masterVector = torch.rand((num_classes, encoder_features))\n",
    "        self.sample_labels = sample_labels\n",
    "        self.bins = []\n",
    "\n",
    "        for i in range(0, num_classes):\n",
    "            self.bins.append(np.where(self.sample_labels == i)[0])\n",
    "\n",
    "    def init_param(self, mean=1e-8, std=1e-9):\n",
    "        torch.nn.init.normal_(self.u, mean=mean, std=std)\n",
    "\n",
    "    def forward(self, index, outputs, label, out, flag, epoch):\n",
    "        if len(outputs) > len(index):\n",
    "            output, output2 = torch.chunk(outputs, 2)\n",
    "            out1, out2 = torch.chunk(out, 2)\n",
    "        else:\n",
    "            output = outputs\n",
    "            out1 = out\n",
    "\n",
    "        eps = 1e-4\n",
    "\n",
    "        u = self.u[index]\n",
    "\n",
    "        if flag == 0:\n",
    "            if self.beginning:\n",
    "                percent = math.ceil((50 - (50 / total_epochs) * epoch) + 50)\n",
    "                for i in range(0, len(self.bins)):\n",
    "                    class_u = self.u.detach()[self.bins[i]]\n",
    "                    bottomK = int((len(class_u) / 100) * percent)\n",
    "                    important_indexs = torch.topk(class_u, bottomK, largest=False, dim=0)[1]\n",
    "                    self.masterVector[i] = torch.mean(\n",
    "                        self.prevSimilarity[self.bins[i]][important_indexs.view(-1)], dim=0\n",
    "                    )\n",
    "\n",
    "            masterVector_norm = self.masterVector.norm(p=2, dim=1, keepdim=True)\n",
    "            masterVector_normalized = self.masterVector.div(masterVector_norm)\n",
    "            self.masterVector_transpose = torch.transpose(masterVector_normalized, 0, 1)\n",
    "            self.beginning = True\n",
    "\n",
    "        self.prevSimilarity[index] = out1.detach()\n",
    "\n",
    "        prediction = F.softmax(output, dim=1)\n",
    "\n",
    "        out_norm = out1.detach().norm(p=2, dim=1, keepdim=True)\n",
    "        out_normalized = out1.detach().div(out_norm)\n",
    "\n",
    "        similarity = torch.mm(out_normalized, self.masterVector_transpose)\n",
    "        similarity = similarity * label\n",
    "        sim_mask = (similarity > 0.000).type(torch.float32)\n",
    "        similarity = similarity * sim_mask\n",
    "\n",
    "        u = u * label\n",
    "\n",
    "        prediction = torch.clamp((prediction + u.detach()), min=eps, max=1.0)\n",
    "        loss = torch.mean(-torch.sum((similarity) * torch.log(prediction), dim=1))\n",
    "\n",
    "        label_one_hot = self.soft_to_hard(output.detach())\n",
    "\n",
    "        MSE_loss = F.mse_loss((label_one_hot + u), label, reduction=\"sum\") / len(label)\n",
    "        loss += MSE_loss\n",
    "\n",
    "        if self.ratio_balance > 0:\n",
    "            avg_prediction = torch.mean(prediction, dim=0)\n",
    "            prior_distr = 1.0 / self.num_classes * torch.ones_like(avg_prediction)\n",
    "\n",
    "            avg_prediction = torch.clamp(avg_prediction, min=eps, max=1.0)\n",
    "\n",
    "            balance_kl = torch.mean(-(prior_distr * torch.log(avg_prediction)).sum(dim=0))\n",
    "\n",
    "            loss += self.ratio_balance * balance_kl\n",
    "\n",
    "        if (len(outputs) > len(index)) and (self.ratio_consistency > 0):\n",
    "            consistency_loss = self.consistency_loss(output, output2)\n",
    "\n",
    "            loss += self.ratio_consistency * torch.mean(consistency_loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def consistency_loss(self, output1, output2):\n",
    "        preds1 = F.softmax(output1, dim=1).detach()\n",
    "        preds2 = F.log_softmax(output2, dim=1)\n",
    "        loss_kldiv = F.kl_div(preds2, preds1, reduction=\"none\")\n",
    "        loss_kldiv = torch.sum(loss_kldiv, dim=1)\n",
    "        return loss_kldiv\n",
    "\n",
    "    def soft_to_hard(self, x):\n",
    "        with torch.no_grad():\n",
    "            return (torch.zeros(len(x), self.num_classes)).cuda().scatter_(1, (x.argmax(dim=1)).view(-1, 1), 1)\n",
    "\n",
    "    def to(self, device):\n",
    "        # transfère data de u sans casser le paramètre\n",
    "        self.u.data = self.u.data.to(device)\n",
    "        # transfère les tensors non-paramètres\n",
    "        self.prevSimilarity = self.prevSimilarity.to(device)\n",
    "        self.masterVector = self.masterVector.to(device)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35cb39e6-aa87-4f8d-9f01-c1bc21258482",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(data_loader, model, optimizer, criterion, device, save_checkpoints, checkpoint_path, current_epoch, optimizer_u=None, ncod_mode=False):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_id, data in enumerate(tqdm(data_loader, desc=\"Iterating training graphs\", unit=\"batch\")):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        if ncod_mode and optimizer_u is not None:\n",
    "            optimizer_u.zero_grad()\n",
    "\n",
    "        # Sortie du modèle : logits, embeddings\n",
    "        output, embeddings = model(data)\n",
    "\n",
    "        # NCOD mode\n",
    "        if isinstance(criterion, NCODLoss):\n",
    "            # Obtenir les indices et les labels one-hot\n",
    "            indices = getattr(data, 'idx', torch.arange(data.y.size(0))).to(device)\n",
    "            indices_cpu = indices.cpu()\n",
    "\n",
    "            one_hot_labels = torch.zeros(len(data.y), criterion.num_classes).to(device)\n",
    "            one_hot_labels.scatter_(1, data.y.view(-1, 1), 1)\n",
    "\n",
    "            # Appel à la fonction de perte NCOD\n",
    "            loss = criterion(indices_cpu, output, one_hot_labels, embeddings, batch_id, current_epoch)\n",
    "        else:\n",
    "            # Perte classique\n",
    "            loss = criterion(output, data.y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if ncod_mode and optimizer_u is not None:\n",
    "            optimizer_u.step()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "        total += data.y.size(0)\n",
    "\n",
    "    # Save checkpoints si nécessaire\n",
    "    if save_checkpoints:\n",
    "        checkpoint_file = f\"{checkpoint_path}_epoch_{current_epoch + 1}.pth\"\n",
    "        torch.save(model.state_dict(), checkpoint_file)\n",
    "        print(f\"Checkpoint saved at {checkpoint_file}\")\n",
    "\n",
    "    return total_loss / len(data_loader), correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1417818",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.dataset[index]\n",
    "        data.idx = torch.tensor([index])  # Ajouter l'index à l'objet `Data`\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c599d3-551a-4472-a59e-b247d3aca645",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:31<00:00,  4.40batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 35/35 [00:06<00:00,  5.50batch/s]\n",
      "Epoch 1/20, Loss: 1.5174, Train Acc: 0.4237, Val Acc: 0.2259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.5174, Train Acc: 0.4237, Val Acc: 0.2259\n",
      "Best model updated and saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/model_B_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:32<00:00,  4.34batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 35/35 [00:06<00:00,  5.76batch/s]\n",
      "Epoch 2/20, Loss: 1.4310, Train Acc: 0.4806, Val Acc: 0.4527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Loss: 1.4310, Train Acc: 0.4806, Val Acc: 0.4527\n",
      "Best model updated and saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/model_B_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:40<00:00,  3.43batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 35/35 [00:05<00:00,  6.28batch/s]\n",
      "Epoch 3/20, Loss: 1.3769, Train Acc: 0.5011, Val Acc: 0.4089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Loss: 1.3769, Train Acc: 0.5011, Val Acc: 0.4089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:32<00:00,  4.34batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 35/35 [00:06<00:00,  5.71batch/s]\n",
      "Epoch 4/20, Loss: 1.3734, Train Acc: 0.4996, Val Acc: 0.4902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Loss: 1.3734, Train Acc: 0.4996, Val Acc: 0.4902\n",
      "Best model updated and saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/model_B_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:32<00:00,  4.32batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 35/35 [00:06<00:00,  5.75batch/s]\n",
      "Epoch 5/20, Loss: 1.3460, Train Acc: 0.5094, Val Acc: 0.5125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Loss: 1.3460, Train Acc: 0.5094, Val Acc: 0.5125\n",
      "Best model updated and saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/model_B_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:32<00:00,  4.27batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/B/model_B_epoch_6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 35/35 [00:06<00:00,  5.32batch/s]\n",
      "Epoch 6/20, Loss: 1.3370, Train Acc: 0.5127, Val Acc: 0.4554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Loss: 1.3370, Train Acc: 0.5127, Val Acc: 0.4554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:32<00:00,  4.27batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 35/35 [00:06<00:00,  5.80batch/s]\n",
      "Epoch 7/20, Loss: 1.3010, Train Acc: 0.5214, Val Acc: 0.4393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Loss: 1.3010, Train Acc: 0.5214, Val Acc: 0.4393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:32<00:00,  4.28batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 35/35 [00:06<00:00,  5.49batch/s]\n",
      "Epoch 8/20, Loss: 1.2856, Train Acc: 0.5239, Val Acc: 0.5027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Loss: 1.2856, Train Acc: 0.5239, Val Acc: 0.5027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:32<00:00,  4.32batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 35/35 [00:06<00:00,  5.58batch/s]\n",
      "Epoch 9/20, Loss: 1.2560, Train Acc: 0.5328, Val Acc: 0.5321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Loss: 1.2560, Train Acc: 0.5328, Val Acc: 0.5321\n",
      "Best model updated and saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/model_B_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:32<00:00,  4.35batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 35/35 [00:06<00:00,  5.76batch/s]\n",
      "Epoch 10/20, Loss: 1.2310, Train Acc: 0.5362, Val Acc: 0.5393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Loss: 1.2310, Train Acc: 0.5362, Val Acc: 0.5393\n",
      "Best model updated and saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/model_B_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:32<00:00,  4.36batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 35/35 [00:06<00:00,  5.54batch/s]\n",
      "Epoch 11/20, Loss: 1.2321, Train Acc: 0.5317, Val Acc: 0.5125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Loss: 1.2321, Train Acc: 0.5317, Val Acc: 0.5125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:32<00:00,  4.33batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 35/35 [00:06<00:00,  5.52batch/s]\n",
      "Epoch 12/20, Loss: 1.1918, Train Acc: 0.5453, Val Acc: 0.5116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Loss: 1.1918, Train Acc: 0.5453, Val Acc: 0.5116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:32<00:00,  4.34batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/B/model_B_epoch_13.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 35/35 [00:06<00:00,  5.43batch/s]\n",
      "Epoch 13/20, Loss: 1.2006, Train Acc: 0.5377, Val Acc: 0.5411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Loss: 1.2006, Train Acc: 0.5377, Val Acc: 0.5411\n",
      "Best model updated and saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/model_B_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:32<00:00,  4.25batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 35/35 [00:06<00:00,  5.58batch/s]\n",
      "Epoch 14/20, Loss: 1.1756, Train Acc: 0.5451, Val Acc: 0.4661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Loss: 1.1756, Train Acc: 0.5451, Val Acc: 0.4661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:32<00:00,  4.32batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 35/35 [00:06<00:00,  5.49batch/s]\n",
      "Epoch 15/20, Loss: 1.1635, Train Acc: 0.5464, Val Acc: 0.5223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Loss: 1.1635, Train Acc: 0.5464, Val Acc: 0.5223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:32<00:00,  4.30batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 35/35 [00:06<00:00,  5.70batch/s]\n",
      "Epoch 16/20, Loss: 1.1592, Train Acc: 0.5446, Val Acc: 0.5312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Loss: 1.1592, Train Acc: 0.5446, Val Acc: 0.5312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:40<00:00,  3.45batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 35/35 [00:05<00:00,  5.83batch/s]\n",
      "Epoch 17/20, Loss: 1.1529, Train Acc: 0.5424, Val Acc: 0.5321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Loss: 1.1529, Train Acc: 0.5424, Val Acc: 0.5321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:32<00:00,  4.36batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 35/35 [00:05<00:00,  5.84batch/s]\n",
      "Epoch 18/20, Loss: 1.1364, Train Acc: 0.5487, Val Acc: 0.5473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Loss: 1.1364, Train Acc: 0.5487, Val Acc: 0.5473\n",
      "Best model updated and saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/model_B_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:31<00:00,  4.40batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 35/35 [00:05<00:00,  5.90batch/s]\n",
      "Epoch 19/20, Loss: 1.1030, Train Acc: 0.5576, Val Acc: 0.5420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Loss: 1.1030, Train Acc: 0.5576, Val Acc: 0.5420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:32<00:00,  4.31batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/B/model_B_epoch_20.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 35/35 [00:06<00:00,  5.65batch/s]\n",
      "Epoch 20/20, Loss: 1.1102, Train Acc: 0.5516, Val Acc: 0.5375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Loss: 1.1102, Train Acc: 0.5516, Val Acc: 0.5375\n"
     ]
    }
   ],
   "source": [
    "if args.train_path:\n",
    "    full_dataset = GraphDataset(args.train_path, transform=add_zeros)\n",
    "    val_size = int(0.2 * len(full_dataset))\n",
    "    train_size = len(full_dataset) - val_size\n",
    "\n",
    "    generator = torch.Generator().manual_seed(12)\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "    train_labels = torch.tensor([data.y.item() for data in train_dataset])\n",
    "\n",
    "    train_dataset = IndexedDataset(train_dataset)  # ajoute les index\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    num_epochs = args.epochs\n",
    "    best_val_accuracy = 0.0 \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Définir les hyperparamètres globaux pour NCOD\n",
    "    encoder_features = args.emb_dim \n",
    "    total_epochs = num_epochs\n",
    "    mean = 1e-8\n",
    "    std = 1e-9\n",
    "\n",
    "    # Définir la loss function \n",
    "    if args.baseline_mode == 2:\n",
    "        criterion = NoisyCrossEntropyLoss(args.noise_prob)\n",
    "        optimizer_u = None  # pas utilisé\n",
    "    elif args.baseline_mode == 3:\n",
    "        num_classes = 6 \n",
    "        num_samples = train_size\n",
    "\n",
    "        criterion = NCODLoss(\n",
    "            sample_labels=train_labels,\n",
    "            num_examp=num_samples,\n",
    "            num_classes=num_classes,\n",
    "            ratio_consistency=1.0,\n",
    "            ratio_balance=0.1).to(device)\n",
    "\n",
    "        # optimiseur séparé pour u \n",
    "        optimizer_u = torch.optim.SGD([criterion.u], lr=0.1, weight_decay=0.0)\n",
    "    else:\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer_u = None\n",
    "\n",
    "    #Training loop\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    if num_checkpoints > 1:\n",
    "        checkpoint_intervals = [int((i + 1) * num_epochs / num_checkpoints) for i in range(num_checkpoints)]\n",
    "    else:\n",
    "        checkpoint_intervals = [num_epochs]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train(\n",
    "            train_loader,\n",
    "            model,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            device,\n",
    "            current_epoch=epoch,\n",
    "            optimizer_u=optimizer_u, \n",
    "            ncod_mode=(args.baseline_mode == 3),  #pour savoir si NCOD est activé\n",
    "            save_checkpoints=(epoch + 1 in checkpoint_intervals),\n",
    "            checkpoint_path=os.path.join(checkpoints_folder, f\"model_{test_dir_name}\")\n",
    "        )\n",
    "\n",
    "        val_loss, val_acc = evaluate(val_loader, model, device, calculate_accuracy=True)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        logging.info(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Best model updated and saved at {checkpoint_path}\")\n",
    "\n",
    "    plot_training_progress(train_losses, train_accuracies, os.path.join(logs_folder, \"plots\"))\n",
    "    plot_training_progress(val_losses, val_accuracies, os.path.join(logs_folder, \"plotsVal\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07594aff",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "del train_dataset\n",
    "del train_loader\n",
    "del full_dataset\n",
    "del val_datasetda\n",
    "del val_loader\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba668fff",
   "metadata": {
    "id": "xsXZIj4Mdu3I",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = GraphDataset(args.test_path, transform=add_zeros)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7828e983",
   "metadata": {
    "id": "x1OnGq_nCmTr",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "predictions = evaluate(test_loader, model, device, calculate_accuracy=False)\n",
    "save_predictions(predictions, args.test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343c36b2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 108.457758,
   "end_time": "2025-05-21T16:25:04.482169",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-21T16:23:16.024411",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
