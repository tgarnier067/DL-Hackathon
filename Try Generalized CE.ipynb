{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "name": "Baseline"
    },
    "accelerator": "TPU",
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric --quiet"
      ],
      "metadata": {
        "id": "xSkgt1zf-raF",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22947ae3-5407-4281-fde5-493c0183c87e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --branch baselineCe https://github.com/Graph-Classification-Noisy-Label/hackaton.git"
      ],
      "metadata": {
        "id": "5oR2D2Us-xSQ",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d789154a-5795-4a6a-f8f3-1bff9be04ef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hackaton'...\n",
            "remote: Enumerating objects: 81, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 81 (delta 7), reused 0 (delta 0), pack-reused 62 (from 2)\u001b[K\n",
            "Receiving objects: 100% (81/81), 105.83 MiB | 48.97 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "%cd hackaton/"
      ],
      "metadata": {
        "id": "tEhfPly6-7UK",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5909b951-dfa1-404f-f024-33ae02b98499"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hackaton\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --folder https://drive.google.com/drive/folders/1Z-1JkPJ6q4C6jX4brvq1VRbJH5RPUCAk -O datasets"
      ],
      "metadata": {
        "id": "PxBvwB0_6xI8",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f547fffd-26c6-4d7f-bcb5-8024f7de5ee5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder contents\n",
            "Retrieving folder 1wcUVBNQkZ04zStXkglXSgERfIvjSHJiL A\n",
            "Processing file 1C8sjkO6JS0j2SyVwQ07m8PhQ-pHpuI78 test.json.gz\n",
            "Processing file 12N11n8gufNA_C1ns-1IeBseBHgrSfRI1 train.json.gz\n",
            "Retrieving folder 1Tj5YoYYDDXjDxxi-cywZgoDkT0b1Qbz- B\n",
            "Processing file 11GBlrXMdP3HSD60w-56Tu6rbGkR-Ifww test.json.gz\n",
            "Processing file 13vp-Kwef3UgAwMG-dokGwKyARym9iqtL train.json.gz\n",
            "Retrieving folder 1e3B_tBMd693Iwv8x3zRR9c47l5yt_5ey C\n",
            "Processing file 18XVe65ZsQ0PDLCqQa4WmneVhyfjGcXmT test.json.gz\n",
            "Processing file 1z5lvG2CytbLQZt7Jmo9BopzFd0pKejEj train.json.gz\n",
            "Retrieving folder 1cvM0eZwpD4gzjo44_zdodxudVBMrLza1 D\n",
            "Processing file 1Gna_dHnBLX8vKaYGAAqAbw5QPerrNK1u test.json.gz\n",
            "Processing file 1Pc-6LMML80-AgEoLVs2Q5hLtmR_rTEek train.json.gz\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1C8sjkO6JS0j2SyVwQ07m8PhQ-pHpuI78\n",
            "From (redirected): https://drive.google.com/uc?id=1C8sjkO6JS0j2SyVwQ07m8PhQ-pHpuI78&confirm=t&uuid=7e09d879-1edb-4d3c-aea7-159bd2f58f1d\n",
            "To: /content/hackaton/datasets/A/test.json.gz\n",
            "100% 92.4M/92.4M [00:00<00:00, 110MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=12N11n8gufNA_C1ns-1IeBseBHgrSfRI1\n",
            "From (redirected): https://drive.google.com/uc?id=12N11n8gufNA_C1ns-1IeBseBHgrSfRI1&confirm=t&uuid=e2c8c30d-6bd7-46fd-a71c-6ab0e06b14aa\n",
            "To: /content/hackaton/datasets/A/train.json.gz\n",
            "100% 465M/465M [00:04<00:00, 100MB/s] \n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=11GBlrXMdP3HSD60w-56Tu6rbGkR-Ifww\n",
            "From (redirected): https://drive.google.com/uc?id=11GBlrXMdP3HSD60w-56Tu6rbGkR-Ifww&confirm=t&uuid=e00e23e5-a8cb-4899-b7e6-947a27714bb7\n",
            "To: /content/hackaton/datasets/B/test.json.gz\n",
            "100% 63.0M/63.0M [00:00<00:00, 197MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=13vp-Kwef3UgAwMG-dokGwKyARym9iqtL\n",
            "From (redirected): https://drive.google.com/uc?id=13vp-Kwef3UgAwMG-dokGwKyARym9iqtL&confirm=t&uuid=16ddb291-689f-47e4-843b-8b63120d756e\n",
            "To: /content/hackaton/datasets/B/train.json.gz\n",
            "100% 223M/223M [00:00<00:00, 228MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=18XVe65ZsQ0PDLCqQa4WmneVhyfjGcXmT\n",
            "From (redirected): https://drive.google.com/uc?id=18XVe65ZsQ0PDLCqQa4WmneVhyfjGcXmT&confirm=t&uuid=c871f2f2-7fb8-4756-a20f-f3d2bf6b84b8\n",
            "To: /content/hackaton/datasets/C/test.json.gz\n",
            "100% 60.5M/60.5M [00:00<00:00, 109MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1z5lvG2CytbLQZt7Jmo9BopzFd0pKejEj\n",
            "From (redirected): https://drive.google.com/uc?id=1z5lvG2CytbLQZt7Jmo9BopzFd0pKejEj&confirm=t&uuid=722d5b68-e4a5-4084-8e6c-c364af9e5bef\n",
            "To: /content/hackaton/datasets/C/train.json.gz\n",
            "100% 308M/308M [00:02<00:00, 113MB/s] \n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1Gna_dHnBLX8vKaYGAAqAbw5QPerrNK1u\n",
            "From (redirected): https://drive.google.com/uc?id=1Gna_dHnBLX8vKaYGAAqAbw5QPerrNK1u&confirm=t&uuid=a3003586-c2b9-4d26-94ed-0adf8de8acbb\n",
            "To: /content/hackaton/datasets/D/test.json.gz\n",
            "100% 94.0M/94.0M [00:00<00:00, 242MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1Pc-6LMML80-AgEoLVs2Q5hLtmR_rTEek\n",
            "From (redirected): https://drive.google.com/uc?id=1Pc-6LMML80-AgEoLVs2Q5hLtmR_rTEek&confirm=t&uuid=4cf088fe-bb0e-4546-bec1-30a8a20b9135\n",
            "To: /content/hackaton/datasets/D/train.json.gz\n",
            "100% 439M/439M [00:04<00:00, 96.0MB/s]\n",
            "Download completed\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh datasets"
      ],
      "metadata": {
        "id": "1rockhiQ7Nny",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40c54fb0-b03d-4726-8582-cda7f8a14de2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 16K\n",
            "drwxr-xr-x 2 root root 4.0K May 23 09:24 A\n",
            "drwxr-xr-x 2 root root 4.0K May 23 09:24 B\n",
            "drwxr-xr-x 2 root root 4.0K May 23 09:24 C\n",
            "drwxr-xr-x 2 root root 4.0K May 23 09:25 D\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "# Load utility functions from cloned repository\n",
        "from src.loadData import GraphDataset\n",
        "from src.utils import set_seed\n",
        "from src.models import GNN\n",
        "import argparse\n",
        "\n",
        "# Set the random seed\n",
        "set_seed()\n"
      ],
      "metadata": {
        "id": "lAQuCuIoBbq5",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "def add_zeros(data):\n",
        "    data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n",
        "    return data"
      ],
      "metadata": {
        "id": "Dyf0I2-t9IcW",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "def train(data_loader, model, optimizer, criterion, device, save_checkpoints, checkpoint_path, current_epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data in tqdm(data_loader, desc=\"Iterating training graphs\", unit=\"batch\"):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += (pred == data.y).sum().item()\n",
        "        total += data.y.size(0)\n",
        "\n",
        "    # Save checkpoints if required\n",
        "    if save_checkpoints:\n",
        "        checkpoint_file = f\"{checkpoint_path}_epoch_{current_epoch + 1}.pth\"\n",
        "        torch.save(model.state_dict(), checkpoint_file)\n",
        "        print(f\"Checkpoint saved at {checkpoint_file}\")\n",
        "\n",
        "    return total_loss / len(data_loader),  correct / total"
      ],
      "metadata": {
        "id": "3jKvoQYI9Zbc",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(data_loader, model, device, calculate_accuracy=False):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    predictions = []\n",
        "    total_loss = 0\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(data_loader, desc=\"Iterating eval graphs\", unit=\"batch\"):\n",
        "            data = data.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1)\n",
        "\n",
        "            if calculate_accuracy:\n",
        "                correct += (pred == data.y).sum().item()\n",
        "                total += data.y.size(0)\n",
        "                total_loss += criterion(output, data.y).item()\n",
        "            else:\n",
        "                predictions.extend(pred.cpu().numpy())\n",
        "    if calculate_accuracy:\n",
        "        accuracy = correct / total\n",
        "        return  total_loss / len(data_loader),accuracy\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "8peFiIS19ZpK",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "def save_predictions(predictions, test_path):\n",
        "    script_dir = os.getcwd()\n",
        "    submission_folder = os.path.join(script_dir, \"submission\")\n",
        "    test_dir_name = os.path.basename(os.path.dirname(test_path))\n",
        "\n",
        "    os.makedirs(submission_folder, exist_ok=True)\n",
        "\n",
        "    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n",
        "\n",
        "    test_graph_ids = list(range(len(predictions)))\n",
        "    output_df = pd.DataFrame({\n",
        "        \"id\": test_graph_ids,\n",
        "        \"pred\": predictions\n",
        "    })\n",
        "\n",
        "    output_df.to_csv(output_csv_path, index=False)\n",
        "    print(f\"Predictions saved to {output_csv_path}\")"
      ],
      "metadata": {
        "id": "WanuZKxy9Zs-",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_progress(train_losses, train_accuracies, output_dir):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, label=\"Training Loss\", color='blue')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss per Epoch')\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color='green')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training Accuracy per Epoch')\n",
        "\n",
        "    # Save plots in the current directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, \"training_progress.png\"))\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "uyHIJS5U9ZzB",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_input(prompt, default=None, required=False, type_cast=str):\n",
        "\n",
        "    while True:\n",
        "        user_input = input(f\"{prompt} [{default}]: \")\n",
        "\n",
        "        if user_input == \"\" and required:\n",
        "            print(\"This field is required. Please enter a value.\")\n",
        "            continue\n",
        "\n",
        "        if user_input == \"\" and default is not None:\n",
        "            return default\n",
        "\n",
        "        if user_input == \"\" and not required:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            return type_cast(user_input)\n",
        "        except ValueError:\n",
        "            print(f\"Invalid input. Please enter a valid {type_cast.__name__}.\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "75CUUCQaLN8z"
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": [
        "def get_arguments():\n",
        "    args = {}\n",
        "    args['train_path'] = get_user_input(\"Path to the training dataset (optional)\")\n",
        "    args['test_path'] = get_user_input(\"Path to the test dataset\", required=True)\n",
        "    args['num_checkpoints'] = get_user_input(\"Number of checkpoints to save during training\", type_cast=int)\n",
        "    args['device'] = get_user_input(\"Which GPU to use if any\", default=1, type_cast=int)\n",
        "    args['gnn'] = get_user_input(\"GNN type (gin, gin-virtual, gcn, gcn-virtual)\", default='gin')\n",
        "    args['drop_ratio'] = get_user_input(\"Dropout ratio\", default=0.0, type_cast=float)\n",
        "    args['num_layer'] = get_user_input(\"Number of GNN message passing layers\", default=5, type_cast=int)\n",
        "    args['emb_dim'] = get_user_input(\"Dimensionality of hidden units in GNNs\", default=300, type_cast=int)\n",
        "    args['batch_size'] = get_user_input(\"Input batch size for training\", default=32, type_cast=int)\n",
        "    args['epochs'] = get_user_input(\"Number of epochs to train\", default=10, type_cast=int)\n",
        "    args['baseline_mode'] = get_user_input(\"Baseline mode: 1 (CE), 2 (Noisy CE), 3 (Generalized CE)\", default=1, type_cast=int)\n",
        "    args['noise_prob'] = get_user_input(\"Noise probability p (used if baseline_mode=2)\", default=0.2, type_cast=float)\n",
        "    args['q_GCE'] = get_user_input(\"q (used if baseline_mode=3)\", default=0.7, type_cast=float)\n",
        "\n",
        "\n",
        "    return argparse.Namespace(**args)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "ltMpKr-uLN8z"
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "def populate_args(args):\n",
        "    print(\"Arguments received:\")\n",
        "    for key, value in vars(args).items():\n",
        "        print(f\"{key}: {value}\")\n",
        "args = get_arguments()\n",
        "populate_args(args)"
      ],
      "metadata": {
        "trusted": true,
        "id": "menGNf0_LN80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34d61b52-6b09-4e6d-b6de-190f835a79e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to the training dataset (optional) [None]: datasets/A/train.json.gz\n",
            "Path to the test dataset [None]: datasets/A/test.json.gz\n",
            "Number of checkpoints to save during training [None]: \n",
            "Which GPU to use if any [1]: \n",
            "GNN type (gin, gin-virtual, gcn, gcn-virtual) [gin]: \n",
            "Dropout ratio [0.0]: \n",
            "Number of GNN message passing layers [5]: \n",
            "Dimensionality of hidden units in GNNs [300]: \n",
            "Input batch size for training [32]: \n",
            "Number of epochs to train [10]: \n",
            "Baseline mode: 1 (CE), 2 (Noisy CE), 3 (Generalized CE) [1]: 3\n",
            "Noise probability p (used if baseline_mode=2) [0.2]: \n",
            "q (used if baseline_mode=3) [0.7]: \n",
            "Arguments received:\n",
            "train_path: datasets/A/train.json.gz\n",
            "test_path: datasets/A/test.json.gz\n",
            "num_checkpoints: None\n",
            "device: 1\n",
            "gnn: gin\n",
            "drop_ratio: 0.0\n",
            "num_layer: 5\n",
            "emb_dim: 300\n",
            "batch_size: 32\n",
            "epochs: 10\n",
            "baseline_mode: 3\n",
            "noise_prob: 0.2\n",
            "q_GCE: 0.7\n"
          ]
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "class NoisyCrossEntropyLoss(torch.nn.Module):\n",
        "    def __init__(self, p_noisy):\n",
        "        super().__init__()\n",
        "        self.p = p_noisy\n",
        "        self.ce = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        losses = self.ce(logits, targets)\n",
        "        weights = (1 - self.p) + self.p * (1 - torch.nn.functional.one_hot(targets, num_classes=logits.size(1)).float().sum(dim=1))\n",
        "        return (losses * weights).mean()"
      ],
      "metadata": {
        "trusted": true,
        "id": "6R78GyYSLN80"
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": [
        "class GeneralizedCrossEntropyLoss(torch.nn.Module):\n",
        "    def __init__(self, q=0.7):  # q hyperparamètre entre 0 et 1\n",
        "        super().__init__()\n",
        "        self.q = q\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        pt = probs.gather(1, targets.unsqueeze(1)).squeeze(1)  # prob des bonnes classes\n",
        "        loss = (1 - pt ** self.q) / self.q\n",
        "        return loss.mean()"
      ],
      "metadata": {
        "id": "QNnuohfNLuMU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "script_dir = os.getcwd()\n",
        "# device = torch.device(f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_checkpoints = args.num_checkpoints if args.num_checkpoints else 3\n",
        "\n",
        "if args.gnn == 'gin':\n",
        "    model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False).to(device)\n",
        "elif args.gnn == 'gin-virtual':\n",
        "    model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True).to(device)\n",
        "elif args.gnn == 'gcn':\n",
        "    model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False).to(device)\n",
        "elif args.gnn == 'gcn-virtual':\n",
        "    model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True).to(device)\n",
        "else:\n",
        "    raise ValueError('Invalid GNN type')\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "# criterion = torch.nn.CrossEntropyLoss()\n",
        "if args.baseline_mode == 2:\n",
        "    criterion = NoisyCrossEntropyLoss(args.noise_prob)\n",
        "elif args.baseline_mode == 3:\n",
        "    criterion = GeneralizedCrossEntropyLoss(args.q_GCE)\n",
        "else:\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "lHX55XGECXBr",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": [
        "test_dir_name = os.path.basename(os.path.dirname(args.test_path))\n",
        "logs_folder = os.path.join(script_dir, \"logs\", test_dir_name)\n",
        "log_file = os.path.join(logs_folder, \"training.log\")\n",
        "os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
        "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(message)s')\n",
        "logging.getLogger().addHandler(logging.StreamHandler())\n",
        "\n",
        "checkpoint_path = os.path.join(script_dir, \"checkpoints\", f\"model_{test_dir_name}_best.pth\")\n",
        "checkpoints_folder = os.path.join(script_dir, \"checkpoints\", test_dir_name)\n",
        "os.makedirs(checkpoints_folder, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "BTYT5jYuChPb",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(checkpoint_path) and not args.train_path:\n",
        "    model.load_state_dict(torch.load(checkpoint_path))\n",
        "    print(f\"Loaded best model from {checkpoint_path}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "o-cUGI4tLN81"
      },
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": [
        "if args.train_path:\n",
        "    full_dataset = GraphDataset(args.train_path, transform=add_zeros)\n",
        "    val_size = int(0.2 * len(full_dataset))\n",
        "    train_size = len(full_dataset) - val_size\n",
        "\n",
        "\n",
        "    generator = torch.Generator().manual_seed(12)\n",
        "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
        "\n",
        "    num_epochs = args.epochs\n",
        "    best_val_accuracy = 0.0\n",
        "\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    if num_checkpoints > 1:\n",
        "        checkpoint_intervals = [int((i + 1) * num_epochs / num_checkpoints) for i in range(num_checkpoints)]\n",
        "    else:\n",
        "        checkpoint_intervals = [num_epochs]\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = train(\n",
        "            train_loader, model, optimizer, criterion, device,\n",
        "            save_checkpoints=(epoch + 1 in checkpoint_intervals),\n",
        "            checkpoint_path=os.path.join(checkpoints_folder, f\"model_{test_dir_name}\"),\n",
        "            current_epoch=epoch\n",
        "        )\n",
        "\n",
        "        val_loss,val_acc = evaluate(val_loader, model, device, calculate_accuracy=True)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "        logging.info(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "\n",
        "        if val_acc > best_val_accuracy:\n",
        "            best_val_accuracy = val_acc\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            print(f\"Best model updated and saved at {checkpoint_path}\")\n",
        "\n",
        "    plot_training_progress(train_losses, train_accuracies, os.path.join(logs_folder, \"plots\"))\n",
        "    plot_training_progress(val_losses, val_accuracies, os.path.join(logs_folder, \"plotsVal\"))"
      ],
      "metadata": {
        "trusted": true,
        "id": "4Wv7_jcDLN82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa4d42a6-bb37-4da0-ff94-46ba61dffcfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iterating training graphs:  46%|████▋     | 131/282 [17:38<21:44,  8.64s/batch]"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "del train_dataset\n",
        "del train_loader\n",
        "del full_dataset\n",
        "del val_dataset\n",
        "del val_loader\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "_p9SEzTKLN82"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = GraphDataset(args.test_path, transform=add_zeros)\n",
        "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "xsXZIj4Mdu3I",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(checkpoint_path))\n",
        "predictions = evaluate(test_loader, model, device, calculate_accuracy=False)\n",
        "save_predictions(predictions, args.test_path)"
      ],
      "metadata": {
        "id": "x1OnGq_nCmTr",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "EjKsdZvPLN83"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}