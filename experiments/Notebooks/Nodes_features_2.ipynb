{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29060e11-1798-4209-8123-87e5c815afd2",
   "metadata": {},
   "source": [
    "On fait comme dans le notebook Node_features, mais au lieu de remplacer les 0 par le degré, on rajoute beaucoup plus d'information !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b3d782-4ce2-423b-8a80-3ca86b1de249",
   "metadata": {},
   "source": [
    "# Change the GNN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b72328d7-0780-4995-ba13-d94ddc50fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch_geometric torch --quiet\n",
    "!pip install networkx --quiet\n",
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "import math\n",
    "\n",
    "### GIN convolution along the graph structure\n",
    "class GINConv(MessagePassing):\n",
    "    def __init__(self, emb_dim):\n",
    "        '''\n",
    "            emb_dim (int): node embedding dimensionality\n",
    "        '''\n",
    "\n",
    "        super(GINConv, self).__init__(aggr = \"add\")\n",
    "\n",
    "        self.mlp = torch.nn.Sequential(torch.nn.Linear(emb_dim, 2*emb_dim), torch.nn.BatchNorm1d(2*emb_dim), torch.nn.ReLU(), torch.nn.Linear(2*emb_dim, emb_dim))\n",
    "        self.eps = torch.nn.Parameter(torch.Tensor([0]))\n",
    "\n",
    "        self.edge_encoder = torch.nn.Linear(7, emb_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        edge_embedding = self.edge_encoder(edge_attr)\n",
    "        out = self.mlp((1 + self.eps) *x + self.propagate(edge_index, x=x, edge_attr=edge_embedding))\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        return F.relu(x_j + edge_attr)\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "\n",
    "### GCN convolution along the graph structure\n",
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, emb_dim):\n",
    "        super(GCNConv, self).__init__(aggr='add')\n",
    "\n",
    "        self.linear = torch.nn.Linear(emb_dim, emb_dim)\n",
    "        self.root_emb = torch.nn.Embedding(1, emb_dim)\n",
    "        self.edge_encoder = torch.nn.Linear(7, emb_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.linear(x)\n",
    "        edge_embedding = self.edge_encoder(edge_attr)\n",
    "\n",
    "        row, col = edge_index\n",
    "\n",
    "        #edge_weight = torch.ones((edge_index.size(1), ), device=edge_index.device)\n",
    "        deg = degree(row, x.size(0), dtype = x.dtype) + 1\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        return self.propagate(edge_index, x=x, edge_attr = edge_embedding, norm=norm) + F.relu(x + self.root_emb.weight) * 1./deg.view(-1,1)\n",
    "\n",
    "    def message(self, x_j, edge_attr, norm):\n",
    "        return norm.view(-1, 1) * F.relu(x_j + edge_attr)\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "\n",
    "\n",
    "### GNN to generate node embedding\n",
    "class GNN_node(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "        node representations\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layer, emb_dim, in_features=4, drop_ratio = 0.5, JK = \"last\", residual = False, gnn_type = 'gin'):\n",
    "        '''\n",
    "            emb_dim (int): node embedding dimensionality\n",
    "            num_layer (int): number of GNN message passing layers\n",
    "\n",
    "        '''\n",
    "\n",
    "        super(GNN_node, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.JK = JK\n",
    "        ### add residual connection or not\n",
    "        self.residual = residual\n",
    "\n",
    "        if self.num_layer < 2:\n",
    "            raise ValueError(\"Number of GNN layers must be greater than 1.\")\n",
    "\n",
    "        self.node_encoder = torch.nn.Linear(in_features, emb_dim) # uniform input node embedding\n",
    "\n",
    "        ###List of GNNs\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "\n",
    "        for layer in range(num_layer):\n",
    "            if gnn_type == 'gin':\n",
    "                self.convs.append(GINConv(emb_dim))\n",
    "            elif gnn_type == 'gcn':\n",
    "                self.convs.append(GCNConv(emb_dim))\n",
    "            else:\n",
    "                raise ValueError('Undefined GNN type called {}'.format(gnn_type))\n",
    "\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(emb_dim))\n",
    "\n",
    "    def forward(self, batched_data):\n",
    "        x, edge_index, edge_attr, batch = batched_data.x, batched_data.edge_index, batched_data.edge_attr, batched_data.batch\n",
    "\n",
    "\n",
    "        ### computing input node embedding\n",
    "\n",
    "        h_list = [self.node_encoder(x)]  # x shape [num_nodes, in_features]\n",
    "        for layer in range(self.num_layer):\n",
    "\n",
    "            h = self.convs[layer](h_list[layer], edge_index, edge_attr)\n",
    "            h = self.batch_norms[layer](h)\n",
    "\n",
    "            if layer == self.num_layer - 1:\n",
    "                #remove relu for the last layer\n",
    "                h = F.dropout(h, self.drop_ratio, training = self.training)\n",
    "            else:\n",
    "                h = F.dropout(F.relu(h), self.drop_ratio, training = self.training)\n",
    "\n",
    "            if self.residual:\n",
    "                h += h_list[layer]\n",
    "\n",
    "            h_list.append(h)\n",
    "\n",
    "        ### Different implementations of Jk-concat\n",
    "        if self.JK == \"last\":\n",
    "            node_representation = h_list[-1]\n",
    "        elif self.JK == \"sum\":\n",
    "            node_representation = 0\n",
    "            for layer in range(self.num_layer + 1):\n",
    "                node_representation += h_list[layer]\n",
    "\n",
    "        return node_representation\n",
    "\n",
    "\n",
    "### Virtual GNN to generate node embedding\n",
    "class GNN_node_Virtualnode(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "        node representations\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layer, emb_dim, in_features=4, drop_ratio = 0.5, JK = \"last\", residual = False, gnn_type = 'gin'):\n",
    "        '''\n",
    "            emb_dim (int): node embedding dimensionality\n",
    "        '''\n",
    "\n",
    "        super(GNN_node_Virtualnode, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.JK = JK\n",
    "        ### add residual connection or not\n",
    "        self.residual = residual\n",
    "\n",
    "        if self.num_layer < 2:\n",
    "            raise ValueError(\"Number of GNN layers must be greater than 1.\")\n",
    "\n",
    "        self.node_encoder = torch.nn.Linear(in_features, emb_dim) # uniform input node embedding\n",
    "\n",
    "        ### set the initial virtual node embedding to 0.\n",
    "        self.virtualnode_embedding = torch.nn.Embedding(1, emb_dim)\n",
    "        torch.nn.init.constant_(self.virtualnode_embedding.weight.data, 0)\n",
    "\n",
    "        ### List of GNNs\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        ### batch norms applied to node embeddings\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "\n",
    "        ### List of MLPs to transform virtual node at every layer\n",
    "        self.mlp_virtualnode_list = torch.nn.ModuleList()\n",
    "\n",
    "        for layer in range(num_layer):\n",
    "            if gnn_type == 'gin':\n",
    "                self.convs.append(GINConv(emb_dim))\n",
    "            elif gnn_type == 'gcn':\n",
    "                self.convs.append(GCNConv(emb_dim))\n",
    "            else:\n",
    "                raise ValueError('Undefined GNN type called {}'.format(gnn_type))\n",
    "\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(emb_dim))\n",
    "\n",
    "        for layer in range(num_layer - 1):\n",
    "            self.mlp_virtualnode_list.append(torch.nn.Sequential(torch.nn.Linear(emb_dim, 2*emb_dim), torch.nn.BatchNorm1d(2*emb_dim), torch.nn.ReLU(), \\\n",
    "                                                    torch.nn.Linear(2*emb_dim, emb_dim), torch.nn.BatchNorm1d(emb_dim), torch.nn.ReLU()))\n",
    "\n",
    "\n",
    "    def forward(self, batched_data):\n",
    "\n",
    "        x, edge_index, edge_attr, batch = batched_data.x, batched_data.edge_index, batched_data.edge_attr, batched_data.batch\n",
    "\n",
    "        ### virtual node embeddings for graphs\n",
    "        virtualnode_embedding = self.virtualnode_embedding(torch.zeros(batch[-1].item() + 1).to(edge_index.dtype).to(edge_index.device))\n",
    "\n",
    "        h_list = [self.node_encoder(x)]  # x shape [num_nodes, in_features]\n",
    "\n",
    "        for layer in range(self.num_layer):\n",
    "            ### add message from virtual nodes to graph nodes\n",
    "            h_list[layer] = h_list[layer] + virtualnode_embedding[batch]\n",
    "\n",
    "            ### Message passing among graph nodes\n",
    "            h = self.convs[layer](h_list[layer], edge_index, edge_attr)\n",
    "\n",
    "            h = self.batch_norms[layer](h)\n",
    "            if layer == self.num_layer - 1:\n",
    "                #remove relu for the last layer\n",
    "                h = F.dropout(h, self.drop_ratio, training = self.training)\n",
    "            else:\n",
    "                h = F.dropout(F.relu(h), self.drop_ratio, training = self.training)\n",
    "\n",
    "            if self.residual:\n",
    "                h = h + h_list[layer]\n",
    "\n",
    "            h_list.append(h)\n",
    "\n",
    "            ### update the virtual nodes\n",
    "            if layer < self.num_layer - 1:\n",
    "                ### add message from graph nodes to virtual nodes\n",
    "                virtualnode_embedding_temp = global_add_pool(h_list[layer], batch) + virtualnode_embedding\n",
    "                ### transform virtual nodes using MLP\n",
    "\n",
    "                if self.residual:\n",
    "                    virtualnode_embedding = virtualnode_embedding + F.dropout(self.mlp_virtualnode_list[layer](virtualnode_embedding_temp), self.drop_ratio, training = self.training)\n",
    "                else:\n",
    "                    virtualnode_embedding = F.dropout(self.mlp_virtualnode_list[layer](virtualnode_embedding_temp), self.drop_ratio, training = self.training)\n",
    "\n",
    "        ### Different implementations of Jk-concat\n",
    "        if self.JK == \"last\":\n",
    "            node_representation = h_list[-1]\n",
    "        elif self.JK == \"sum\":\n",
    "            node_representation = 0\n",
    "            for layer in range(self.num_layer + 1):\n",
    "                node_representation += h_list[layer]\n",
    "\n",
    "        return node_representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8860c5a5-81a3-4781-ab2d-f2f4322a29d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import global_add_pool, global_mean_pool, global_max_pool, GlobalAttention, Set2Set\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.inits import uniform\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_class, num_layer = 5, emb_dim = 300, \n",
    "                    gnn_type = 'gin', virtual_node = True, residual = False, drop_ratio = 0.5, JK = \"last\", graph_pooling = \"mean\"):\n",
    "        '''\n",
    "            num_tasks (int): number of labels to be predicted\n",
    "            virtual_node (bool): whether to add virtual node or not\n",
    "        '''\n",
    "\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.num_layer = num_layer\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.JK = JK\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_class = num_class\n",
    "        self.graph_pooling = graph_pooling\n",
    "\n",
    "        if self.num_layer < 2:\n",
    "            raise ValueError(\"Number of GNN layers must be greater than 1.\")\n",
    "\n",
    "        ### GNN to generate node embeddings\n",
    "        if virtual_node:\n",
    "            self.gnn_node = GNN_node_Virtualnode(num_layer, emb_dim, in_features=4, JK=JK, drop_ratio=drop_ratio, residual=residual, gnn_type=gnn_type)\n",
    "\n",
    "        else:\n",
    "            self.gnn_node = GNN_node(num_layer, emb_dim, in_features=4, JK=JK, drop_ratio=drop_ratio, residual=residual, gnn_type=gnn_type)\n",
    "\n",
    "\n",
    "        ### Pooling function to generate whole-graph embeddings\n",
    "        if self.graph_pooling == \"sum\":\n",
    "            self.pool = global_add_pool\n",
    "        elif self.graph_pooling == \"mean\":\n",
    "            self.pool = global_mean_pool\n",
    "        elif self.graph_pooling == \"max\":\n",
    "            self.pool = global_max_pool\n",
    "        elif self.graph_pooling == \"attention\":\n",
    "            self.pool = GlobalAttention(gate_nn = torch.nn.Sequential(torch.nn.Linear(emb_dim, 2*emb_dim), torch.nn.BatchNorm1d(2*emb_dim), torch.nn.ReLU(), torch.nn.Linear(2*emb_dim, 1)))\n",
    "        elif self.graph_pooling == \"set2set\":\n",
    "            self.pool = Set2Set(emb_dim, processing_steps = 2)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid graph pooling type.\")\n",
    "\n",
    "        if graph_pooling == \"set2set\":\n",
    "            self.graph_pred_linear = torch.nn.Linear(2*self.emb_dim, self.num_class)\n",
    "        else:\n",
    "            self.graph_pred_linear = torch.nn.Linear(self.emb_dim, self.num_class)\n",
    "\n",
    "    def forward(self, batched_data):\n",
    "        h_node = self.gnn_node(batched_data)\n",
    "\n",
    "        h_graph = self.pool(h_node, batched_data.batch)\n",
    "\n",
    "        return self.graph_pred_linear(h_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c11eff70-53d7-427e-8211-2e1007deed5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/onyxia/work/DL-Hackathon/hackaton\n"
     ]
    }
   ],
   "source": [
    "%cd hackaton/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3699acda-01f2-44c1-a7a3-b81a0d35a620",
   "metadata": {},
   "source": [
    "# Check the changements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c854abd-2b9e-4780-ae52-6853102353c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import degree\n",
    "\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "def add_topological_features(data):\n",
    "    # Convertir en graphe networkx (non orienté)\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "    # Calcul des mesures nodales\n",
    "    degree_dict = dict(G.degree())\n",
    "    clustering_dict = nx.clustering(G)\n",
    "    degree_centrality_dict = nx.degree_centrality(G)\n",
    "    closeness_centrality_dict = nx.closeness_centrality(G)\n",
    "\n",
    "    # Récupérer les valeurs dans l'ordre des noeuds (0..N-1)\n",
    "    deg = torch.tensor([degree_dict[i] for i in range(data.num_nodes)], dtype=torch.float).view(-1,1)\n",
    "    clustering = torch.tensor([clustering_dict[i] for i in range(data.num_nodes)], dtype=torch.float).view(-1,1)\n",
    "    deg_cent = torch.tensor([degree_centrality_dict[i] for i in range(data.num_nodes)], dtype=torch.float).view(-1,1)\n",
    "    close_cent = torch.tensor([closeness_centrality_dict[i] for i in range(data.num_nodes)], dtype=torch.float).view(-1,1)\n",
    "\n",
    "    # Concaténation en feature nodale de dimension 4\n",
    "    data.x = torch.cat([deg, clustering, deg_cent, close_cent], dim=1)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d1e6173-fdca-40d4-9986-6a7c444fd34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.loadData import GraphDataset\n",
    "    \n",
    "full_dataset = GraphDataset('datasets/A/train.json.gz', transform=add_topological_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56959a62-b656-45c8-8be8-343649d881ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphDataset(11280)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b349fa7-2b2a-4990-a054-d6dfa5098a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 3544], edge_attr=[3544, 7], y=[1], num_nodes=300, x=[300, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46abb290-881e-4411-aba9-ba4ed3574345",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.0000e+00, 8.6667e-01, 2.0067e-02, 2.3035e-01],\n",
       "        [6.0000e+00, 5.3333e-01, 2.0067e-02, 2.7108e-01],\n",
       "        [5.0000e+00, 1.0000e+00, 1.6722e-02, 2.3018e-01],\n",
       "        ...,\n",
       "        [2.0000e+00, 1.0000e+00, 6.6890e-03, 2.9751e-01],\n",
       "        [2.0000e+00, 1.0000e+00, 6.6890e-03, 2.5754e-01],\n",
       "        [1.0000e+00, 0.0000e+00, 3.3445e-03, 2.4548e-01]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset[0].x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba784f3c-2494-4942-b046-a82f26b9c94f",
   "metadata": {},
   "source": [
    "# Try the changement on co teaching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5b11355-2325-4d35-bb64-b351c2f727d0",
   "metadata": {
    "id": "lAQuCuIoBbq5",
    "papermill": {
     "duration": 9.949638,
     "end_time": "2025-05-21T16:25:02.510764",
     "exception": false,
     "start_time": "2025-05-21T16:24:52.561126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "# Load utility functions from cloned repository\n",
    "from src.loadData import GraphDataset\n",
    "from src.utils import set_seed\n",
    "#from src.models import GNN (We do not need it, as we changed it in the above cells)\n",
    "import argparse\n",
    "\n",
    "# Set the random seed\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9042e8d6-2197-4946-a003-3a429454f27a",
   "metadata": {
    "id": "Dyf0I2-t9IcW",
    "papermill": {
     "duration": 0.019268,
     "end_time": "2025-05-21T16:25:02.544583",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.525315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_topological_features(data):\n",
    "    # Convertir en graphe networkx (non orienté)\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "    # Calcul des mesures nodales\n",
    "    degree_dict = dict(G.degree())\n",
    "    clustering_dict = nx.clustering(G)\n",
    "    degree_centrality_dict = nx.degree_centrality(G)\n",
    "    closeness_centrality_dict = nx.closeness_centrality(G)\n",
    "\n",
    "    # Récupérer les valeurs dans l'ordre des noeuds (0..N-1)\n",
    "    deg = torch.tensor([degree_dict[i] for i in range(data.num_nodes)], dtype=torch.float).view(-1,1)\n",
    "    clustering = torch.tensor([clustering_dict[i] for i in range(data.num_nodes)], dtype=torch.float).view(-1,1)\n",
    "    deg_cent = torch.tensor([degree_centrality_dict[i] for i in range(data.num_nodes)], dtype=torch.float).view(-1,1)\n",
    "    close_cent = torch.tensor([closeness_centrality_dict[i] for i in range(data.num_nodes)], dtype=torch.float).view(-1,1)\n",
    "\n",
    "    # Concaténation en feature nodale de dimension 4\n",
    "    data.x = torch.cat([deg, clustering, deg_cent, close_cent], dim=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea206014-c22c-4204-8fcb-9920f8a05e5f",
   "metadata": {
    "id": "3jKvoQYI9Zbc",
    "papermill": {
     "duration": 0.019599,
     "end_time": "2025-05-21T16:25:02.577661",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.558062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(data_loader, model, optimizer, criterion, device, save_checkpoints, checkpoint_path, current_epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in tqdm(data_loader, desc=\"Iterating training graphs\", unit=\"batch\"):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "        total += data.y.size(0)\n",
    "\n",
    "    # Save checkpoints if required\n",
    "    if save_checkpoints:\n",
    "        checkpoint_file = f\"{checkpoint_path}_epoch_{current_epoch + 1}.pth\"\n",
    "        torch.save(model.state_dict(), checkpoint_file)\n",
    "        print(f\"Checkpoint saved at {checkpoint_file}\")\n",
    "\n",
    "    return total_loss / len(data_loader),  correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ab77352-c742-4d60-b7df-e2134a9713e0",
   "metadata": {
    "id": "8peFiIS19ZpK",
    "papermill": {
     "duration": 0.017908,
     "end_time": "2025-05-21T16:25:02.607848",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.589940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(data_loader, model, device, calculate_accuracy=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    total_loss = 0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader, desc=\"Iterating eval graphs\", unit=\"batch\"):\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            \n",
    "            if calculate_accuracy:\n",
    "                correct += (pred == data.y).sum().item()\n",
    "                total += data.y.size(0)\n",
    "                total_loss += criterion(output, data.y).item()\n",
    "            else:\n",
    "                predictions.extend(pred.cpu().numpy())\n",
    "    if calculate_accuracy:\n",
    "        accuracy = correct / total\n",
    "        return  total_loss / len(data_loader),accuracy\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2907c85e-22e3-42da-99ed-3d566c037183",
   "metadata": {
    "id": "WanuZKxy9Zs-",
    "papermill": {
     "duration": 0.016728,
     "end_time": "2025-05-21T16:25:02.635694",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.618966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_predictions(predictions, test_path):\n",
    "    script_dir = os.getcwd() \n",
    "    submission_folder = os.path.join(script_dir, \"submission\")\n",
    "    test_dir_name = os.path.basename(os.path.dirname(test_path))\n",
    "    \n",
    "    os.makedirs(submission_folder, exist_ok=True)\n",
    "    \n",
    "    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n",
    "    \n",
    "    test_graph_ids = list(range(len(predictions)))\n",
    "    output_df = pd.DataFrame({\n",
    "        \"id\": test_graph_ids,\n",
    "        \"pred\": predictions\n",
    "    })\n",
    "    \n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b773f13b-d061-46eb-824a-bb5b22749205",
   "metadata": {
    "id": "uyHIJS5U9ZzB",
    "papermill": {
     "duration": 0.017765,
     "end_time": "2025-05-21T16:25:02.664538",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.646773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_training_progress(train_losses, train_accuracies, output_dir):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Training Loss\", color='blue')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss per Epoch')\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color='green')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training Accuracy per Epoch')\n",
    "\n",
    "    # Save plots in the current directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"training_progress.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d2a0056-e04b-465a-b260-1295d4b199ea",
   "metadata": {
    "papermill": {
     "duration": 0.016577,
     "end_time": "2025-05-21T16:25:02.692205",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.675628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_user_input(prompt, default=None, required=False, type_cast=str):\n",
    "\n",
    "    while True:\n",
    "        user_input = input(f\"{prompt} [{default}]: \")\n",
    "        \n",
    "        if user_input == \"\" and required:\n",
    "            print(\"This field is required. Please enter a value.\")\n",
    "            continue\n",
    "        \n",
    "        if user_input == \"\" and default is not None:\n",
    "            return default\n",
    "        \n",
    "        if user_input == \"\" and not required:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            return type_cast(user_input)\n",
    "        except ValueError:\n",
    "            print(f\"Invalid input. Please enter a valid {type_cast.__name__}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41a1e5af-90ea-417e-a794-32817106d688",
   "metadata": {
    "papermill": {
     "duration": 0.017703,
     "end_time": "2025-05-21T16:25:02.721184",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.703481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_arguments():\n",
    "    args = {}\n",
    "    args['train_path'] = get_user_input(\"Path to the training dataset (optional)\")\n",
    "    args['test_path'] = get_user_input(\"Path to the test dataset\", required=True)\n",
    "    args['num_checkpoints'] = get_user_input(\"Number of checkpoints to save during training\", type_cast=int)\n",
    "    args['device'] = get_user_input(\"Which GPU to use if any\", default=1, type_cast=int)\n",
    "    args['gnn'] = get_user_input(\"GNN type (gin, gin-virtual, gcn, gcn-virtual)\", default='gin')\n",
    "    args['res'] = get_user_input(\"Residuals in GNN ? (1 yes, 0 no)\", default=0)\n",
    "    args['drop_ratio'] = get_user_input(\"Dropout ratio\", default=0.0, type_cast=float)\n",
    "    args['num_layer'] = get_user_input(\"Number of GNN message passing layers\", default=5, type_cast=int)\n",
    "    args['emb_dim'] = get_user_input(\"Dimensionality of hidden units in GNNs\", default=300, type_cast=int)\n",
    "    args['batch_size'] = get_user_input(\"Input batch size for training\", default=32, type_cast=int)\n",
    "    args['epochs'] = get_user_input(\"Number of epochs to train\", default=10, type_cast=int)\n",
    "    args['baseline_mode'] = get_user_input(\"Baseline mode: 1 (CE), 2 (Noisy CE)\", default=1, type_cast=int)\n",
    "    args['noise_prob'] = get_user_input(\"Noise probability p (used if baseline_mode=2)\", default=0.2, type_cast=float)\n",
    "    args['pooling'] = get_user_input(\"type of pooling (sum, mean, max, attention, set2set)\", default='mean')\n",
    "    \n",
    "    return argparse.Namespace(**args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8fb41b2-bfaf-41c1-ba33-5514397e9393",
   "metadata": {
    "papermill": {
     "duration": 0.118164,
     "end_time": "2025-05-21T16:25:02.850799",
     "exception": true,
     "start_time": "2025-05-21T16:25:02.732635",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Path to the training dataset (optional) [None]:  datasets/A/train.json.gz\n",
      "Path to the test dataset [None]:  datasets/A/test.json.gz\n",
      "Number of checkpoints to save during training [None]:  10\n",
      "Which GPU to use if any [1]:  1\n",
      "GNN type (gin, gin-virtual, gcn, gcn-virtual) [gin]:  gcn\n",
      "Residuals in GNN ? (1 yes, 0 no) [0]:  1\n",
      "Dropout ratio [0.0]:  0.2\n",
      "Number of GNN message passing layers [5]:  \n",
      "Dimensionality of hidden units in GNNs [300]:  \n",
      "Input batch size for training [32]:  \n",
      "Number of epochs to train [10]:  \n",
      "Baseline mode: 1 (CE), 2 (Noisy CE) [1]:  \n",
      "Noise probability p (used if baseline_mode=2) [0.2]:  \n",
      "type of pooling (sum, mean, max, attention, set2set) [mean]:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments received:\n",
      "train_path: datasets/A/train.json.gz\n",
      "test_path: datasets/A/test.json.gz\n",
      "num_checkpoints: 10\n",
      "device: 1\n",
      "gnn: gcn\n",
      "res: 1\n",
      "drop_ratio: 0.2\n",
      "num_layer: 5\n",
      "emb_dim: 300\n",
      "batch_size: 32\n",
      "epochs: 10\n",
      "baseline_mode: 1\n",
      "noise_prob: 0.2\n",
      "pooling: mean\n"
     ]
    }
   ],
   "source": [
    "def populate_args(args):\n",
    "    print(\"Arguments received:\")\n",
    "    for key, value in vars(args).items():\n",
    "        print(f\"{key}: {value}\")\n",
    "args = get_arguments()\n",
    "populate_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df1ed4eb-e818-4a19-8faf-cc6604f1af34",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NoisyCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, p_noisy):\n",
    "        super().__init__()\n",
    "        self.p = p_noisy\n",
    "        self.ce = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        losses = self.ce(logits, targets)\n",
    "        weights = (1 - self.p) + self.p * (1 - torch.nn.functional.one_hot(targets, num_classes=logits.size(1)).float().sum(dim=1))\n",
    "        return (losses * weights).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edc932c2-6e93-40da-be3d-10e03c350049",
   "metadata": {
    "id": "lHX55XGECXBr",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "script_dir = os.getcwd() \n",
    "# device = torch.device(f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_checkpoints = args.num_checkpoints if args.num_checkpoints else 3\n",
    "    \n",
    "if args.gnn == 'gin':\n",
    "    model1 = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False, residual = True if args.res == 1 else False, graph_pooling=args.pooling).to(device)\n",
    "    model2 = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False, residual = True if args.res == 1 else False, graph_pooling=args.pooling).to(device)\n",
    "elif args.gnn == 'gin-virtual':\n",
    "    model1 = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True, residual = True if args.res == 1 else False, graph_pooling=args.pooling).to(device)\n",
    "    model2 = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True, residual = True if args.res == 1 else False, graph_pooling=args.pooling).to(device)\n",
    "elif args.gnn == 'gcn':\n",
    "    model1 = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False, residual = True if args.res == 1 else False, graph_pooling=args.pooling).to(device)\n",
    "    model2 = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False, residual = True if args.res == 1 else False, graph_pooling=args.pooling).to(device)\n",
    "elif args.gnn == 'gcn-virtual':\n",
    "    model1 = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True, residual = True if args.res == 1 else False, graph_pooling=args.pooling).to(device)\n",
    "    model2 = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True, residual = True if args.res == 1 else False, graph_pooling=args.pooling).to(device)\n",
    "else:\n",
    "    raise ValueError('Invalid GNN type')\n",
    "    \n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "if args.baseline_mode == 2:\n",
    "    criterion = NoisyCrossEntropyLoss(args.noise_prob)\n",
    "else:\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac8b7d06-70e6-4f1c-826a-419d70871f84",
   "metadata": {
    "id": "BTYT5jYuChPb",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dir_name = os.path.basename(os.path.dirname(args.test_path))\n",
    "logs_folder = os.path.join(script_dir, \"logs\", test_dir_name)\n",
    "log_file = os.path.join(logs_folder, \"training.log\")\n",
    "os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logging.getLogger().addHandler(logging.StreamHandler())\n",
    "\n",
    "checkpoint_path = os.path.join(script_dir, \"checkpoints\", f\"model_{test_dir_name}_best.pth\")\n",
    "checkpoints_folder = os.path.join(script_dir, \"checkpoints\", test_dir_name)\n",
    "os.makedirs(checkpoints_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "116d310f-fed8-45e4-8ded-3ac161e4dc87",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists(checkpoint_path) and not args.train_path:\n",
    "    model1.load_state_dict(torch.load(checkpoint_path))\n",
    "    print(f\"Loaded best model from {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffa6e4ce-4ce0-44d4-91d2-a5572edb9c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_coteaching(train_loader, model1, model2, optimizer1, optimizer2, criterion, device, forget_rate, epoch, num_epochs):\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "\n",
    "    total_correct1 = total_correct2 = total_samples = 0\n",
    "    running_loss1 = 0.0\n",
    "    running_loss2 = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device, non_blocking=True)\n",
    "\n",
    "        out1 = model1(batch)\n",
    "        out2 = model2(batch)\n",
    "\n",
    "        # Perte par échantillon (shape: [batch_size])\n",
    "        loss1 = criterion(out1, batch.y)\n",
    "        loss2 = criterion(out2, batch.y)\n",
    "\n",
    "        remember_rate = 1.0 - forget_rate\n",
    "        num_remember = int(remember_rate * batch.y.size(0))\n",
    "\n",
    "        _, idx1 = loss1.topk(num_remember, largest=False)\n",
    "        _, idx2 = loss2.topk(num_remember, largest=False)\n",
    "\n",
    "        loss1_update = criterion(out1[idx2], batch.y[idx2]).mean()\n",
    "        loss2_update = criterion(out2[idx1], batch.y[idx1]).mean()\n",
    "\n",
    "        optimizer1.zero_grad()\n",
    "        loss1_update.backward()\n",
    "        optimizer1.step()\n",
    "\n",
    "        optimizer2.zero_grad()\n",
    "        loss2_update.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "        # On calcule la loss moyenne pondérée par la taille du batch (pour l’epoch)\n",
    "        running_loss1 += loss1_update.item() * batch.y.size(0)\n",
    "        running_loss2 += loss2_update.item() * batch.y.size(0)\n",
    "\n",
    "        # Calcul des bonnes prédictions\n",
    "        total_correct1 += (out1.argmax(dim=1) == batch.y).sum().item()\n",
    "        total_correct2 += (out2.argmax(dim=1) == batch.y).sum().item()\n",
    "        total_samples += batch.y.size(0)\n",
    "\n",
    "    avg_loss1 = running_loss1 / total_samples\n",
    "    avg_loss2 = running_loss2 / total_samples\n",
    "    acc1 = total_correct1 / total_samples\n",
    "    acc2 = total_correct2 / total_samples\n",
    "\n",
    "    return acc1, acc2, avg_loss1, avg_loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6a35777-ddf2-4e7f-98f8-525a2363c206",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 71/71 [03:44<00:00,  3.16s/batch]\n",
      "Iterating eval graphs: 100%|██████████| 71/71 [04:07<00:00,  3.49s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Forget rate: 0.000\n",
      "Train Loss Model1: 1.5812, Acc: 0.3871\n",
      "Train Loss Model2: 1.5864, Acc: 0.3844\n",
      "Val   Loss Model1: 1.5055, Acc: 0.4242\n",
      "Val   Loss Model2: 1.5478, Acc: 0.4109\n",
      "[Co-Teaching] Best model1 updated and saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/A/model_A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m forget_rate = get_forget_rate(epoch, num_epochs)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Phase d'entraînement\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m train_acc1, train_acc2, train_loss1, train_loss2 = \u001b[43mtrain_coteaching\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforget_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Phase de validation\u001b[39;00m\n\u001b[32m     43\u001b[39m val_loss1, val_acc1 = evaluate(val_loader, model1, device, calculate_accuracy=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mtrain_coteaching\u001b[39m\u001b[34m(train_loader, model1, model2, optimizer1, optimizer2, criterion, device, forget_rate, epoch, num_epochs)\u001b[39m\n\u001b[32m      6\u001b[39m running_loss1 = \u001b[32m0.0\u001b[39m\n\u001b[32m      7\u001b[39m running_loss2 = \u001b[32m0.0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout1\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/utils/data/dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch_geometric/data/dataset.py:292\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, (\u001b[38;5;28mint\u001b[39m, np.integer))\n\u001b[32m    288\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, Tensor) \u001b[38;5;129;01mand\u001b[39;00m idx.dim() == \u001b[32m0\u001b[39m)\n\u001b[32m    289\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, np.ndarray) \u001b[38;5;129;01mand\u001b[39;00m np.isscalar(idx))):\n\u001b[32m    291\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.get(\u001b[38;5;28mself\u001b[39m.indices()[idx])\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     data = data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36madd_topological_features\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m      7\u001b[39m clustering_dict = nx.clustering(G)\n\u001b[32m      8\u001b[39m degree_centrality_dict = nx.degree_centrality(G)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m closeness_centrality_dict = \u001b[43mnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcloseness_centrality\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Récupérer les valeurs dans l'ordre des noeuds (0..N-1)\u001b[39;00m\n\u001b[32m     12\u001b[39m deg = torch.tensor([degree_dict[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(data.num_nodes)], dtype=torch.float).view(-\u001b[32m1\u001b[39m,\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<class 'networkx.utils.decorators.argmap'> compilation 16:3\u001b[39m, in \u001b[36margmap_closeness_centrality_13\u001b[39m\u001b[34m(G, u, distance, wf_improved, backend, **backend_kwargs)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbz2\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgzip\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minspect\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitertools\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/networkx/utils/backends.py:967\u001b[39m, in \u001b[36m_dispatchable.__call__\u001b[39m\u001b[34m(self, backend, *args, **kwargs)\u001b[39m\n\u001b[32m    965\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m backend != \u001b[33m\"\u001b[39m\u001b[33mnetworkx\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    966\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m backend is not installed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m967\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43morig_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[38;5;66;03m# Use `backend_name` in this function instead of `backend`.\u001b[39;00m\n\u001b[32m    970\u001b[39m \u001b[38;5;66;03m# This is purely for aesthetics and to make it easier to search for this\u001b[39;00m\n\u001b[32m    971\u001b[39m \u001b[38;5;66;03m# variable since \"backend\" is used in many comments and log/error messages.\u001b[39;00m\n\u001b[32m    972\u001b[39m backend_name = backend\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/networkx/algorithms/centrality/closeness.py:124\u001b[39m, in \u001b[36mcloseness_centrality\u001b[39m\u001b[34m(G, u, distance, wf_improved)\u001b[39m\n\u001b[32m    122\u001b[39m closeness_dict = {}\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     sp = \u001b[43mpath_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m     totsp = \u001b[38;5;28msum\u001b[39m(sp.values())\n\u001b[32m    126\u001b[39m     len_G = \u001b[38;5;28mlen\u001b[39m(G)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<class 'networkx.utils.decorators.argmap'> compilation 20:3\u001b[39m, in \u001b[36margmap_single_source_shortest_path_length_17\u001b[39m\u001b[34m(G, source, cutoff, backend, **backend_kwargs)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbz2\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgzip\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minspect\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitertools\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/networkx/utils/backends.py:967\u001b[39m, in \u001b[36m_dispatchable.__call__\u001b[39m\u001b[34m(self, backend, *args, **kwargs)\u001b[39m\n\u001b[32m    965\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m backend != \u001b[33m\"\u001b[39m\u001b[33mnetworkx\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    966\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m backend is not installed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m967\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43morig_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[38;5;66;03m# Use `backend_name` in this function instead of `backend`.\u001b[39;00m\n\u001b[32m    970\u001b[39m \u001b[38;5;66;03m# This is purely for aesthetics and to make it easier to search for this\u001b[39;00m\n\u001b[32m    971\u001b[39m \u001b[38;5;66;03m# variable since \"backend\" is used in many comments and log/error messages.\u001b[39;00m\n\u001b[32m    972\u001b[39m backend_name = backend\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/networkx/algorithms/shortest_paths/unweighted.py:63\u001b[39m, in \u001b[36msingle_source_shortest_path_length\u001b[39m\u001b[34m(G, source, cutoff)\u001b[39m\n\u001b[32m     61\u001b[39m     cutoff = \u001b[38;5;28mfloat\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33minf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     62\u001b[39m nextlevel = [source]\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_single_shortest_path_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_adj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnextlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcutoff\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/networkx/algorithms/shortest_paths/unweighted.py:94\u001b[39m, in \u001b[36m_single_shortest_path_length\u001b[39m\u001b[34m(adj, firstlevel, cutoff)\u001b[39m\n\u001b[32m     92\u001b[39m         seen.add(w)\n\u001b[32m     93\u001b[39m         nextlevel.append(w)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m (w, level)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(seen) == n:\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if args.train_path:\n",
    "    full_dataset = GraphDataset(args.train_path, transform=add_topological_features)\n",
    "    val_size = int(0.2 * len(full_dataset))\n",
    "    train_size = len(full_dataset) - val_size\n",
    "\n",
    "    generator = torch.Generator().manual_seed(12)\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    num_epochs = args.epochs\n",
    "    best_val_accuracy = 0.0\n",
    "\n",
    "    train_losses_model1 = []\n",
    "    train_losses_model2 = []\n",
    "    train_accuracies1 = []\n",
    "    train_accuracies2 = []\n",
    "    val_losses_model1 = []\n",
    "    val_losses_model2 = []\n",
    "    val_accuracies1 = []\n",
    "    val_accuracies2 = []\n",
    "\n",
    "\n",
    "    if num_checkpoints > 1:\n",
    "        checkpoint_intervals = [int((i + 1) * num_epochs / num_checkpoints) for i in range(num_checkpoints)]\n",
    "    else:\n",
    "        checkpoint_intervals = [num_epochs]\n",
    "\n",
    "    def get_forget_rate(epoch, total_epochs, max_forget=0.2):\n",
    "        return min(max_forget, max_forget * epoch / (total_epochs * 0.5))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        forget_rate = get_forget_rate(epoch, num_epochs)\n",
    "\n",
    "        # Phase d'entraînement\n",
    "        train_acc1, train_acc2, train_loss1, train_loss2 = train_coteaching(\n",
    "            train_loader, model1, model2, optimizer1, optimizer2,\n",
    "            criterion, device, forget_rate, epoch, num_epochs\n",
    "        )\n",
    "\n",
    "        # Phase de validation\n",
    "        val_loss1, val_acc1 = evaluate(val_loader, model1, device, calculate_accuracy=True)\n",
    "        val_loss2, val_acc2 = evaluate(val_loader, model2, device, calculate_accuracy=True)\n",
    "\n",
    "        # Stocker les métriques\n",
    "        train_losses_model1.append(train_loss1)\n",
    "        train_losses_model2.append(train_loss2)\n",
    "        train_accuracies1.append(train_acc1)\n",
    "        train_accuracies2.append(train_acc2)\n",
    "    \n",
    "        val_losses_model1.append(val_loss1)\n",
    "        val_losses_model2.append(val_loss2)\n",
    "        val_accuracies1.append(val_acc1)\n",
    "        val_accuracies2.append(val_acc2)\n",
    "    \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Forget rate: {forget_rate:.3f}\")\n",
    "        print(f\"Train Loss Model1: {train_loss1:.4f}, Acc: {train_acc1:.4f}\")\n",
    "        print(f\"Train Loss Model2: {train_loss2:.4f}, Acc: {train_acc2:.4f}\")\n",
    "        print(f\"Val   Loss Model1: {val_loss1:.4f}, Acc: {val_acc1:.4f}\")\n",
    "        print(f\"Val   Loss Model2: {val_loss2:.4f}, Acc: {val_acc2:.4f}\")\n",
    "        \n",
    "        if val_acc1 > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc1\n",
    "            torch.save(model1.state_dict(), os.path.join(checkpoints_folder, f\"model_{test_dir_name}\"))\n",
    "            print(f\"[Co-Teaching] Best model1 updated and saved at {checkpoints_folder}/model_{test_dir_name}\")\n",
    "\n",
    "    # Assuming you have train_losses1 and train_losses2 collected similarly to train_accuracies\n",
    "    plot_training_progress(train_losses_model1, train_accuracies1, os.path.join(logs_folder, \"plots_model1\"))\n",
    "    plot_training_progress(val_losses_model1, val_accuracies1, os.path.join(logs_folder, \"plotsVal_model1\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
