{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xSkgt1zf-raF"
   },
   "outputs": [],
   "source": [
    "!pip install torch_geometric torch gdown  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5oR2D2Us-xSQ",
    "outputId": "bf80f2b3-fd8a-47ca-a75a-5b95b131dfd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'hackaton'...\n",
      "remote: Enumerating objects: 81, done.\u001b[K\n",
      "remote: Counting objects:   5% (1/19)\u001b[K\r",
      "remote: Counting objects:  10% (2/19)\u001b[K\r",
      "remote: Counting objects:  15% (3/19)\u001b[K\r",
      "remote: Counting objects:  21% (4/19)\u001b[K\r",
      "remote: Counting objects:  26% (5/19)\u001b[K\r",
      "remote: Counting objects:  31% (6/19)\u001b[K\r",
      "remote: Counting objects:  36% (7/19)\u001b[K\r",
      "remote: Counting objects:  42% (8/19)\u001b[K\r",
      "remote: Counting objects:  47% (9/19)\u001b[K\r",
      "remote: Counting objects:  52% (10/19)\u001b[K\r",
      "remote: Counting objects:  57% (11/19)\u001b[K\r",
      "remote: Counting objects:  63% (12/19)\u001b[K\r",
      "remote: Counting objects:  68% (13/19)\u001b[K\r",
      "remote: Counting objects:  73% (14/19)\u001b[K\r",
      "remote: Counting objects:  78% (15/19)\u001b[K\r",
      "remote: Counting objects:  84% (16/19)\u001b[K\r",
      "remote: Counting objects:  89% (17/19)\u001b[K\r",
      "remote: Counting objects:  94% (18/19)\u001b[K\r",
      "remote: Counting objects: 100% (19/19)\u001b[K\r",
      "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
      "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
      "remote: Total 81 (delta 7), reused 0 (delta 0), pack-reused 62 (from 2)\u001b[K\n",
      "Receiving objects: 100% (81/81), 105.83 MiB | 45.52 MiB/s, done.\n",
      "Resolving deltas: 100% (8/8), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone --branch baselineCe https://github.com/Graph-Classification-Noisy-Label/hackaton.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tEhfPly6-7UK",
    "outputId": "44be2ba2-1f9b-47d7-e49d-520bdf495958"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/onyxia/work/DL-Hackathon/hackaton\n"
     ]
    }
   ],
   "source": [
    "%cd hackaton/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PxBvwB0_6xI8",
    "outputId": "5fcf4da3-ef44-4acd-fac5-3aa8259987a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents\n",
      "Retrieving folder 1wcUVBNQkZ04zStXkglXSgERfIvjSHJiL A\n",
      "Processing file 1C8sjkO6JS0j2SyVwQ07m8PhQ-pHpuI78 test.json.gz\n",
      "Processing file 12N11n8gufNA_C1ns-1IeBseBHgrSfRI1 train.json.gz\n",
      "Retrieving folder 1Tj5YoYYDDXjDxxi-cywZgoDkT0b1Qbz- B\n",
      "Processing file 11GBlrXMdP3HSD60w-56Tu6rbGkR-Ifww test.json.gz\n",
      "Processing file 13vp-Kwef3UgAwMG-dokGwKyARym9iqtL train.json.gz\n",
      "Retrieving folder 1e3B_tBMd693Iwv8x3zRR9c47l5yt_5ey C\n",
      "Processing file 18XVe65ZsQ0PDLCqQa4WmneVhyfjGcXmT test.json.gz\n",
      "Processing file 1z5lvG2CytbLQZt7Jmo9BopzFd0pKejEj train.json.gz\n",
      "Retrieving folder 1cvM0eZwpD4gzjo44_zdodxudVBMrLza1 D\n",
      "Processing file 1Gna_dHnBLX8vKaYGAAqAbw5QPerrNK1u test.json.gz\n",
      "Processing file 1Pc-6LMML80-AgEoLVs2Q5hLtmR_rTEek train.json.gz\n",
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1C8sjkO6JS0j2SyVwQ07m8PhQ-pHpuI78\n",
      "From (redirected): https://drive.google.com/uc?id=1C8sjkO6JS0j2SyVwQ07m8PhQ-pHpuI78&confirm=t&uuid=c09808ef-7980-4169-b1c2-f759dd392672\n",
      "To: /content/hackaton/datasets/A/test.json.gz\n",
      "100% 92.4M/92.4M [00:01<00:00, 56.5MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=12N11n8gufNA_C1ns-1IeBseBHgrSfRI1\n",
      "From (redirected): https://drive.google.com/uc?id=12N11n8gufNA_C1ns-1IeBseBHgrSfRI1&confirm=t&uuid=2ee384e2-aad6-46e9-bd1d-0f1eec9ca677\n",
      "To: /content/hackaton/datasets/A/train.json.gz\n",
      "100% 465M/465M [00:01<00:00, 258MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=11GBlrXMdP3HSD60w-56Tu6rbGkR-Ifww\n",
      "From (redirected): https://drive.google.com/uc?id=11GBlrXMdP3HSD60w-56Tu6rbGkR-Ifww&confirm=t&uuid=b20a73d5-8d48-43d1-9c02-4dc9a2aaf607\n",
      "To: /content/hackaton/datasets/B/test.json.gz\n",
      "100% 63.0M/63.0M [00:00<00:00, 236MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=13vp-Kwef3UgAwMG-dokGwKyARym9iqtL\n",
      "From (redirected): https://drive.google.com/uc?id=13vp-Kwef3UgAwMG-dokGwKyARym9iqtL&confirm=t&uuid=f163559d-a357-4987-af4e-ab29ca71dfef\n",
      "To: /content/hackaton/datasets/B/train.json.gz\n",
      "100% 223M/223M [00:00<00:00, 284MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=18XVe65ZsQ0PDLCqQa4WmneVhyfjGcXmT\n",
      "From (redirected): https://drive.google.com/uc?id=18XVe65ZsQ0PDLCqQa4WmneVhyfjGcXmT&confirm=t&uuid=8c3512bd-d1a4-48b5-9611-86d38a2715a0\n",
      "To: /content/hackaton/datasets/C/test.json.gz\n",
      "100% 60.5M/60.5M [00:00<00:00, 105MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1z5lvG2CytbLQZt7Jmo9BopzFd0pKejEj\n",
      "From (redirected): https://drive.google.com/uc?id=1z5lvG2CytbLQZt7Jmo9BopzFd0pKejEj&confirm=t&uuid=eea6e361-ccec-4007-98f0-9fc11f160c9e\n",
      "To: /content/hackaton/datasets/C/train.json.gz\n",
      "100% 308M/308M [00:01<00:00, 294MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1Gna_dHnBLX8vKaYGAAqAbw5QPerrNK1u\n",
      "From (redirected): https://drive.google.com/uc?id=1Gna_dHnBLX8vKaYGAAqAbw5QPerrNK1u&confirm=t&uuid=f2087cfa-cf5d-4dff-9cba-ed037ad5ac14\n",
      "To: /content/hackaton/datasets/D/test.json.gz\n",
      "100% 94.0M/94.0M [00:00<00:00, 211MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1Pc-6LMML80-AgEoLVs2Q5hLtmR_rTEek\n",
      "From (redirected): https://drive.google.com/uc?id=1Pc-6LMML80-AgEoLVs2Q5hLtmR_rTEek&confirm=t&uuid=fc516625-763d-4d76-a797-13bec36086f9\n",
      "To: /content/hackaton/datasets/D/train.json.gz\n",
      "100% 439M/439M [00:01<00:00, 229MB/s]\n",
      "Download completed\n"
     ]
    }
   ],
   "source": [
    "!gdown --folder https://drive.google.com/drive/folders/1Z-1JkPJ6q4C6jX4brvq1VRbJH5RPUCAk -O datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1rockhiQ7Nny",
    "outputId": "fe33929a-7fa3-4a9e-a68f-7099efa648cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16K\n",
      "drwxr-sr-x 2 onyxia users 4.0K May 24 07:10 A\n",
      "drwxr-sr-x 2 onyxia users 4.0K May 24 07:11 B\n",
      "drwxr-sr-x 2 onyxia users 4.0K May 24 07:11 C\n",
      "drwxr-sr-x 2 onyxia users 4.0K May 24 07:11 D\n"
     ]
    }
   ],
   "source": [
    "!ls -lh datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lAQuCuIoBbq5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "# Load utility functions from cloned repository\n",
    "from src.loadData import GraphDataset\n",
    "from src.utils import set_seed\n",
    "from src.models import GNN\n",
    "import argparse\n",
    "\n",
    "# Set the random seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Dyf0I2-t9IcW"
   },
   "outputs": [],
   "source": [
    "def add_zeros(data):\n",
    "    data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3jKvoQYI9Zbc"
   },
   "outputs": [],
   "source": [
    "def train(data_loader, model, optimizer, criterion, device, save_checkpoints, checkpoint_path, current_epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in tqdm(data_loader, desc=\"Iterating training graphs\", unit=\"batch\"):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "        total += data.y.size(0)\n",
    "\n",
    "    # Save checkpoints if required\n",
    "    if save_checkpoints:\n",
    "        checkpoint_file = f\"{checkpoint_path}_epoch_{current_epoch + 1}.pth\"\n",
    "        torch.save(model.state_dict(), checkpoint_file)\n",
    "        print(f\"Checkpoint saved at {checkpoint_file}\")\n",
    "\n",
    "    return total_loss / len(data_loader),  correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8peFiIS19ZpK"
   },
   "outputs": [],
   "source": [
    "def evaluate(data_loader, model, device, calculate_accuracy=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    total_loss = 0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader, desc=\"Iterating eval graphs\", unit=\"batch\"):\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "\n",
    "            if calculate_accuracy:\n",
    "                correct += (pred == data.y).sum().item()\n",
    "                total += data.y.size(0)\n",
    "                total_loss += criterion(output, data.y).item()\n",
    "            else:\n",
    "                predictions.extend(pred.cpu().numpy())\n",
    "    if calculate_accuracy:\n",
    "        accuracy = correct / total\n",
    "        return  total_loss / len(data_loader),accuracy\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WanuZKxy9Zs-"
   },
   "outputs": [],
   "source": [
    "def save_predictions(predictions, test_path):\n",
    "    script_dir = os.getcwd()\n",
    "    submission_folder = os.path.join(script_dir, \"submission\")\n",
    "    test_dir_name = os.path.basename(os.path.dirname(test_path))\n",
    "\n",
    "    os.makedirs(submission_folder, exist_ok=True)\n",
    "\n",
    "    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n",
    "\n",
    "    test_graph_ids = list(range(len(predictions)))\n",
    "    output_df = pd.DataFrame({\n",
    "        \"id\": test_graph_ids,\n",
    "        \"pred\": predictions\n",
    "    })\n",
    "\n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uyHIJS5U9ZzB"
   },
   "outputs": [],
   "source": [
    "def plot_training_progress(train_losses, train_accuracies, output_dir):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Training Loss\", color='blue')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss per Epoch')\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color='green')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training Accuracy per Epoch')\n",
    "\n",
    "    # Save plots in the current directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"training_progress.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "75CUUCQaLN8z"
   },
   "outputs": [],
   "source": [
    "def get_user_input(prompt, default=None, required=False, type_cast=str):\n",
    "\n",
    "    while True:\n",
    "        user_input = input(f\"{prompt} [{default}]: \")\n",
    "\n",
    "        if user_input == \"\" and required:\n",
    "            print(\"This field is required. Please enter a value.\")\n",
    "            continue\n",
    "\n",
    "        if user_input == \"\" and default is not None:\n",
    "            return default\n",
    "\n",
    "        if user_input == \"\" and not required:\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            return type_cast(user_input)\n",
    "        except ValueError:\n",
    "            print(f\"Invalid input. Please enter a valid {type_cast.__name__}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ltMpKr-uLN8z"
   },
   "outputs": [],
   "source": [
    "def get_arguments():\n",
    "    args = {}\n",
    "    args['train_path'] = get_user_input(\"Path to the training dataset (optional)\")\n",
    "    args['test_path'] = get_user_input(\"Path to the test dataset\", required=True)\n",
    "    args['num_checkpoints'] = get_user_input(\"Number of checkpoints to save during training\", type_cast=int)\n",
    "    args['device'] = get_user_input(\"Which GPU to use if any\", default=1, type_cast=int)\n",
    "    args['gnn'] = get_user_input(\"GNN type (gin, gin-virtual, gcn, gcn-virtual)\", default='gin')\n",
    "    args['drop_ratio'] = get_user_input(\"Dropout ratio\", default=0.0, type_cast=float)\n",
    "    args['num_layer'] = get_user_input(\"Number of GNN message passing layers\", default=5, type_cast=int)\n",
    "    args['emb_dim'] = get_user_input(\"Dimensionality of hidden units in GNNs\", default=300, type_cast=int)\n",
    "    args['batch_size'] = get_user_input(\"Input batch size for training\", default=32, type_cast=int)\n",
    "    args['epochs'] = get_user_input(\"Number of epochs to train\", default=10, type_cast=int)\n",
    "    args['baseline_mode'] = get_user_input(\"Baseline mode: 1 (CE), 2 (Noisy CE), 3 (Generalized CE)\", default=1, type_cast=int)\n",
    "    args['noise_prob'] = get_user_input(\"Noise probability p (used if baseline_mode=2)\", default=0.2, type_cast=float)\n",
    "    args['q_GCE'] = get_user_input(\"q (used if baseline_mode=3)\", default=0.7, type_cast=float)\n",
    "\n",
    "\n",
    "    return argparse.Namespace(**args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "menGNf0_LN80",
    "outputId": "5b06bd55-9aea-4d3c-b477-20738002eacc"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Path to the training dataset (optional) [None]:  datasets/B/train.json.gz\n",
      "Path to the test dataset [None]:  datasets/B/test.json.gz\n",
      "Number of checkpoints to save during training [None]:  10\n",
      "Which GPU to use if any [1]:  1\n",
      "GNN type (gin, gin-virtual, gcn, gcn-virtual) [gin]:  gcn\n",
      "Dropout ratio [0.0]:  0.2\n",
      "Number of GNN message passing layers [5]:  \n",
      "Dimensionality of hidden units in GNNs [300]:  \n",
      "Input batch size for training [32]:  \n",
      "Number of epochs to train [10]:  10\n",
      "Baseline mode: 1 (CE), 2 (Noisy CE), 3 (Generalized CE) [1]:  1\n",
      "Noise probability p (used if baseline_mode=2) [0.2]:  \n",
      "q (used if baseline_mode=3) [0.7]:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments received:\n",
      "train_path: datasets/B/train.json.gz\n",
      "test_path: datasets/B/test.json.gz\n",
      "num_checkpoints: 10\n",
      "device: 1\n",
      "gnn: gcn\n",
      "drop_ratio: 0.2\n",
      "num_layer: 5\n",
      "emb_dim: 300\n",
      "batch_size: 32\n",
      "epochs: 10\n",
      "baseline_mode: 1\n",
      "noise_prob: 0.2\n",
      "q_GCE: 0.7\n"
     ]
    }
   ],
   "source": [
    "def populate_args(args):\n",
    "    print(\"Arguments received:\")\n",
    "    for key, value in vars(args).items():\n",
    "        print(f\"{key}: {value}\")\n",
    "args = get_arguments()\n",
    "populate_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6R78GyYSLN80"
   },
   "outputs": [],
   "source": [
    "class NoisyCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, p_noisy):\n",
    "        super().__init__()\n",
    "        self.p = p_noisy\n",
    "        self.ce = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        losses = self.ce(logits, targets)\n",
    "        weights = (1 - self.p) + self.p * (1 - torch.nn.functional.one_hot(targets, num_classes=logits.size(1)).float().sum(dim=1))\n",
    "        return (losses * weights).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "QNnuohfNLuMU"
   },
   "outputs": [],
   "source": [
    "class GeneralizedCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, q=0.7):  # q hyperparamètre entre 0 et 1\n",
    "        super().__init__()\n",
    "        self.q = q\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        pt = probs.gather(1, targets.unsqueeze(1)).squeeze(1)  # prob des bonnes classes\n",
    "        loss = (1 - pt ** self.q) / self.q\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "lHX55XGECXBr"
   },
   "outputs": [],
   "source": [
    "script_dir = os.getcwd()\n",
    "# device = torch.device(f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_checkpoints = args.num_checkpoints if args.num_checkpoints else 3\n",
    "\n",
    "if args.gnn == 'gin':\n",
    "    model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False).to(device)\n",
    "elif args.gnn == 'gin-virtual':\n",
    "    model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True).to(device)\n",
    "elif args.gnn == 'gcn':\n",
    "    model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False).to(device)\n",
    "elif args.gnn == 'gcn-virtual':\n",
    "    model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True).to(device)\n",
    "else:\n",
    "    raise ValueError('Invalid GNN type')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "if args.baseline_mode == 2:\n",
    "    criterion = NoisyCrossEntropyLoss(args.noise_prob)\n",
    "elif args.baseline_mode == 3:\n",
    "    criterion = GeneralizedCrossEntropyLoss(args.q_GCE)\n",
    "else:\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BTYT5jYuChPb"
   },
   "outputs": [],
   "source": [
    "test_dir_name = os.path.basename(os.path.dirname(args.test_path))\n",
    "logs_folder = os.path.join(script_dir, \"logs\", test_dir_name)\n",
    "log_file = os.path.join(logs_folder, \"training.log\")\n",
    "os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logging.getLogger().addHandler(logging.StreamHandler())\n",
    "\n",
    "checkpoint_path = os.path.join(script_dir, \"checkpoints\", f\"model_{test_dir_name}_best.pth\")\n",
    "checkpoints_folder = os.path.join(script_dir, \"checkpoints\", test_dir_name)\n",
    "os.makedirs(checkpoints_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "o-cUGI4tLN81"
   },
   "outputs": [],
   "source": [
    "if os.path.exists(checkpoint_path) and not args.train_path:\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    print(f\"Loaded best model from {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Wv7_jcDLN82",
    "outputId": "455ea370-6778-4692-9816-c72b2460b611"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "Iterating training graphs: 100%|██████████| 140/140 [01:23<00:00,  1.68batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/B/model_B_epoch_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 35/35 [00:13<00:00,  2.57batch/s]\n",
      "Epoch 1/10, Loss: 1.6878, Train Acc: 0.3656, Val Acc: 0.2054\n",
      "Epoch 1/10, Loss: 1.6878, Train Acc: 0.3656, Val Acc: 0.2054\n",
      "Epoch 1/10, Loss: 1.6878, Train Acc: 0.3656, Val Acc: 0.2054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.6878, Train Acc: 0.3656, Val Acc: 0.2054\n",
      "Best model updated and saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/model_B_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [01:19<00:00,  1.76batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/B/model_B_epoch_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 35/35 [00:09<00:00,  3.80batch/s]\n",
      "Epoch 2/10, Loss: 1.5790, Train Acc: 0.4379, Val Acc: 0.2902\n",
      "Epoch 2/10, Loss: 1.5790, Train Acc: 0.4379, Val Acc: 0.2902\n",
      "Epoch 2/10, Loss: 1.5790, Train Acc: 0.4379, Val Acc: 0.2902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 1.5790, Train Acc: 0.4379, Val Acc: 0.2902\n",
      "Best model updated and saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/model_B_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:47<00:00,  2.96batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/B/model_B_epoch_3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 35/35 [00:08<00:00,  4.16batch/s]\n",
      "Epoch 3/10, Loss: 1.5337, Train Acc: 0.4638, Val Acc: 0.3036\n",
      "Epoch 3/10, Loss: 1.5337, Train Acc: 0.4638, Val Acc: 0.3036\n",
      "Epoch 3/10, Loss: 1.5337, Train Acc: 0.4638, Val Acc: 0.3036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 1.5337, Train Acc: 0.4638, Val Acc: 0.3036\n",
      "Best model updated and saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/model_B_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:47<00:00,  2.92batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/B/model_B_epoch_4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 35/35 [00:08<00:00,  4.29batch/s]\n",
      "Epoch 4/10, Loss: 1.5179, Train Acc: 0.4656, Val Acc: 0.3098\n",
      "Epoch 4/10, Loss: 1.5179, Train Acc: 0.4656, Val Acc: 0.3098\n",
      "Epoch 4/10, Loss: 1.5179, Train Acc: 0.4656, Val Acc: 0.3098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 1.5179, Train Acc: 0.4656, Val Acc: 0.3098\n",
      "Best model updated and saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/model_B_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:48<00:00,  2.89batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/B/model_B_epoch_5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 35/35 [00:09<00:00,  3.62batch/s]\n",
      "Epoch 5/10, Loss: 1.4951, Train Acc: 0.4830, Val Acc: 0.4625\n",
      "Epoch 5/10, Loss: 1.4951, Train Acc: 0.4830, Val Acc: 0.4625\n",
      "Epoch 5/10, Loss: 1.4951, Train Acc: 0.4830, Val Acc: 0.4625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 1.4951, Train Acc: 0.4830, Val Acc: 0.4625\n",
      "Best model updated and saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/model_B_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:46<00:00,  3.04batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/B/model_B_epoch_6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 35/35 [00:08<00:00,  4.25batch/s]\n",
      "Epoch 6/10, Loss: 1.4651, Train Acc: 0.4989, Val Acc: 0.4714\n",
      "Epoch 6/10, Loss: 1.4651, Train Acc: 0.4989, Val Acc: 0.4714\n",
      "Epoch 6/10, Loss: 1.4651, Train Acc: 0.4989, Val Acc: 0.4714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 1.4651, Train Acc: 0.4989, Val Acc: 0.4714\n",
      "Best model updated and saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/model_B_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:58<00:00,  2.37batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/B/model_B_epoch_7.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 35/35 [00:07<00:00,  4.52batch/s]\n",
      "Epoch 7/10, Loss: 1.4653, Train Acc: 0.5018, Val Acc: 0.3420\n",
      "Epoch 7/10, Loss: 1.4653, Train Acc: 0.5018, Val Acc: 0.3420\n",
      "Epoch 7/10, Loss: 1.4653, Train Acc: 0.5018, Val Acc: 0.3420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 1.4653, Train Acc: 0.5018, Val Acc: 0.3420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:45<00:00,  3.09batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/B/model_B_epoch_8.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 35/35 [00:07<00:00,  4.92batch/s]\n",
      "Epoch 8/10, Loss: 1.4316, Train Acc: 0.5125, Val Acc: 0.4795\n",
      "Epoch 8/10, Loss: 1.4316, Train Acc: 0.5125, Val Acc: 0.4795\n",
      "Epoch 8/10, Loss: 1.4316, Train Acc: 0.5125, Val Acc: 0.4795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 1.4316, Train Acc: 0.5125, Val Acc: 0.4795\n",
      "Best model updated and saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/model_B_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:32<00:00,  4.32batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/B/model_B_epoch_9.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 35/35 [00:06<00:00,  5.47batch/s]\n",
      "Epoch 9/10, Loss: 1.4207, Train Acc: 0.5188, Val Acc: 0.4991\n",
      "Epoch 9/10, Loss: 1.4207, Train Acc: 0.5188, Val Acc: 0.4991\n",
      "Epoch 9/10, Loss: 1.4207, Train Acc: 0.5188, Val Acc: 0.4991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 1.4207, Train Acc: 0.5188, Val Acc: 0.4991\n",
      "Best model updated and saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/model_B_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 140/140 [00:32<00:00,  4.27batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/B/model_B_epoch_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 35/35 [00:06<00:00,  5.45batch/s]\n",
      "Epoch 10/10, Loss: 1.4009, Train Acc: 0.5297, Val Acc: 0.4795\n",
      "Epoch 10/10, Loss: 1.4009, Train Acc: 0.5297, Val Acc: 0.4795\n",
      "Epoch 10/10, Loss: 1.4009, Train Acc: 0.5297, Val Acc: 0.4795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 1.4009, Train Acc: 0.5297, Val Acc: 0.4795\n"
     ]
    }
   ],
   "source": [
    "if args.train_path:\n",
    "    full_dataset = GraphDataset(args.train_path, transform=add_zeros)\n",
    "    val_size = int(0.2 * len(full_dataset))\n",
    "    train_size = len(full_dataset) - val_size\n",
    "\n",
    "\n",
    "    generator = torch.Generator().manual_seed(12)\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    num_epochs = args.epochs\n",
    "    best_val_accuracy = 0.0\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    if num_checkpoints > 1:\n",
    "        checkpoint_intervals = [int((i + 1) * num_epochs / num_checkpoints) for i in range(num_checkpoints)]\n",
    "    else:\n",
    "        checkpoint_intervals = [num_epochs]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train(\n",
    "            train_loader, model, optimizer, criterion, device,\n",
    "            save_checkpoints=(epoch + 1 in checkpoint_intervals),\n",
    "            checkpoint_path=os.path.join(checkpoints_folder, f\"model_{test_dir_name}\"),\n",
    "            current_epoch=epoch\n",
    "        )\n",
    "\n",
    "        val_loss,val_acc = evaluate(val_loader, model, device, calculate_accuracy=True)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        logging.info(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Best model updated and saved at {checkpoint_path}\")\n",
    "\n",
    "    plot_training_progress(train_losses, train_accuracies, os.path.join(logs_folder, \"plots\"))\n",
    "    plot_training_progress(val_losses, val_accuracies, os.path.join(logs_folder, \"plotsVal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_p9SEzTKLN82",
    "outputId": "ba7388de-a3a6-47d9-ece3-7abd4bba4ad0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13426"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del train_dataset\n",
    "del train_loader\n",
    "del full_dataset\n",
    "del val_dataset\n",
    "del val_loader\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "xsXZIj4Mdu3I"
   },
   "outputs": [],
   "source": [
    "test_dataset = GraphDataset(args.test_path, transform=add_zeros)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x1OnGq_nCmTr",
    "outputId": "98b4e41b-b1f8-40d4-e845-096a3d7c85ba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-10583d8b995c>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(checkpoint_path))\n",
      "Iterating eval graphs: 100%|██████████| 147/147 [01:26<00:00,  1.70batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to /content/hackaton/submission/testset_A.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "predictions = evaluate(test_loader, model, device, calculate_accuracy=False)\n",
    "save_predictions(predictions, args.test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJ19M0cB4aTz"
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Opu07CJr4cWI",
    "outputId": "a98c603e-879e-4f98-d100-e3c772a243fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '/content/hackaton/submission' has been compressed into '/content/hackaton/submission.gz'\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "def gzip_folder(folder_path, output_file):\n",
    "    \"\"\"\n",
    "    Compresses an entire folder into a single .tar.gz file.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder to compress.\n",
    "        output_file (str): Path to the output .gz file.\n",
    "    \"\"\"\n",
    "    with tarfile.open(output_file, \"w:gz\") as tar:\n",
    "        tar.add(folder_path, arcname=os.path.basename(folder_path))\n",
    "    print(f\"Folder '{folder_path}' has been compressed into '{output_file}'\")\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"/content/hackaton/submission\"            # Path to the folder you want to compress\n",
    "output_file = \"/content/hackaton/submission.gz\"        # Output .gz file name\n",
    "gzip_folder(folder_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "GAt5V1lN5HDv",
    "outputId": "37dd4e2d-6a93-43d7-b818-68267c409e20"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_7c261adc-c851-47ff-aaa6-1a84d37edcd0\", \"submission.gz\", 16587)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download the folder that we have to submit on HuggingFace\n",
    "from google.colab import files\n",
    "files.download(output_file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "name": "Baseline",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
