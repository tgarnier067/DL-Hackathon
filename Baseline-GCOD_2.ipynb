{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6228bc1c",
   "metadata": {
    "id": "xSkgt1zf-raF",
    "outputId": "59f4a52f-5eb4-41e5-9fba-07432989fe78",
    "papermill": {
     "duration": 5.620104,
     "end_time": "2025-05-21T16:23:24.361690",
     "exception": false,
     "start_time": "2025-05-21T16:23:18.741586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch_geometric torch gdown --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c86aceda",
   "metadata": {
    "id": "5oR2D2Us-xSQ",
    "outputId": "7086cadf-a7fe-4d75-f271-6339bee8164d",
    "papermill": {
     "duration": 4.049235,
     "end_time": "2025-05-21T16:23:28.415556",
     "exception": false,
     "start_time": "2025-05-21T16:23:24.366321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'hackaton'...\n",
      "remote: Enumerating objects: 81, done.\u001b[K\n",
      "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 81 (delta 4), reused 4 (delta 4), pack-reused 70 (from 1)\u001b[K\n",
      "Receiving objects: 100% (81/81), 105.83 MiB | 44.21 MiB/s, done.\n",
      "Resolving deltas: 100% (13/13), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone --branch baselineCe https://github.com/Graph-Classification-Noisy-Label/hackaton.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c129091c",
   "metadata": {
    "id": "tEhfPly6-7UK",
    "outputId": "3078ee06-6312-4fca-f5f9-888fa628c80a",
    "papermill": {
     "duration": 0.013251,
     "end_time": "2025-05-21T16:23:28.434119",
     "exception": false,
     "start_time": "2025-05-21T16:23:28.420868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/onyxia/work/DL-Hackathon/hackaton\n"
     ]
    }
   ],
   "source": [
    "%cd hackaton/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d48103c0",
   "metadata": {
    "id": "PxBvwB0_6xI8",
    "outputId": "5933387c-2cfb-474f-d842-f36a3e2d2a73",
    "papermill": {
     "duration": 83.957666,
     "end_time": "2025-05-21T16:24:52.396720",
     "exception": false,
     "start_time": "2025-05-21T16:23:28.439054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents\n",
      "Retrieving folder 1wcUVBNQkZ04zStXkglXSgERfIvjSHJiL A\n",
      "Processing file 1C8sjkO6JS0j2SyVwQ07m8PhQ-pHpuI78 test.json.gz\n",
      "Processing file 12N11n8gufNA_C1ns-1IeBseBHgrSfRI1 train.json.gz\n",
      "Retrieving folder 1Tj5YoYYDDXjDxxi-cywZgoDkT0b1Qbz- B\n",
      "Processing file 11GBlrXMdP3HSD60w-56Tu6rbGkR-Ifww test.json.gz\n",
      "Processing file 13vp-Kwef3UgAwMG-dokGwKyARym9iqtL train.json.gz\n",
      "Retrieving folder 1e3B_tBMd693Iwv8x3zRR9c47l5yt_5ey C\n",
      "Processing file 18XVe65ZsQ0PDLCqQa4WmneVhyfjGcXmT test.json.gz\n",
      "Processing file 1z5lvG2CytbLQZt7Jmo9BopzFd0pKejEj train.json.gz\n",
      "Retrieving folder 1cvM0eZwpD4gzjo44_zdodxudVBMrLza1 D\n",
      "Processing file 1Gna_dHnBLX8vKaYGAAqAbw5QPerrNK1u test.json.gz\n",
      "Processing file 1Pc-6LMML80-AgEoLVs2Q5hLtmR_rTEek train.json.gz\n",
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Failed to retrieve file url:\n",
      "\n",
      "\tToo many users have viewed or downloaded this file recently. Please\n",
      "\ttry accessing the file again later. If the file you are trying to\n",
      "\taccess is particularly large or is shared with many people, it may\n",
      "\ttake up to 24 hours to be able to view or download the file. If you\n",
      "\tstill can't access a file after 24 hours, contact your domain\n",
      "\tadministrator.\n",
      "\n",
      "You may still be able to access the file from the browser:\n",
      "\n",
      "\thttps://drive.google.com/uc?id=1C8sjkO6JS0j2SyVwQ07m8PhQ-pHpuI78\n",
      "\n",
      "but Gdown can't. Please check connections and permissions.\n"
     ]
    }
   ],
   "source": [
    "!gdown --folder https://drive.google.com/drive/folders/1Z-1JkPJ6q4C6jX4brvq1VRbJH5RPUCAk -O datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80dab1bc",
   "metadata": {
    "id": "1rockhiQ7Nny",
    "outputId": "2cd2e6f4-5f8f-4a62-f0ec-53e6fc78c9b7",
    "papermill": {
     "duration": 0.138969,
     "end_time": "2025-05-21T16:24:52.549260",
     "exception": false,
     "start_time": "2025-05-21T16:24:52.410291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0K\n",
      "drwxr-sr-x 2 onyxia users 4.0K May 28 15:57 A\n"
     ]
    }
   ],
   "source": [
    "!ls -lh datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "817b1078",
   "metadata": {
    "id": "lAQuCuIoBbq5",
    "papermill": {
     "duration": 9.949638,
     "end_time": "2025-05-21T16:25:02.510764",
     "exception": false,
     "start_time": "2025-05-21T16:24:52.561126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "# Load utility functions from cloned repository\n",
    "from src.loadData import GraphDataset\n",
    "from src.utils import set_seed\n",
    "from src.models import GNN\n",
    "import argparse\n",
    "\n",
    "# Set the random seed\n",
    "set_seed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0a9c70d7",
   "metadata": {
    "id": "Dyf0I2-t9IcW",
    "papermill": {
     "duration": 0.019268,
     "end_time": "2025-05-21T16:25:02.544583",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.525315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_zeros(data):\n",
    "    data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61babe5-4786-4543-b885-7f67079a053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCODLoss(torch.nn.Module):\n",
    "    def __init__(self, gamma=0.2):\n",
    "        super(GCODLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        ce_loss = F.cross_entropy(logits, labels, reduction='none')\n",
    "\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        true_probs = probs[range(len(labels)), labels]\n",
    "\n",
    "        weight = (true_probs.detach() ** self.gamma)\n",
    "\n",
    "        loss = weight * ce_loss\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3622cfa1",
   "metadata": {
    "id": "3jKvoQYI9Zbc",
    "papermill": {
     "duration": 0.019599,
     "end_time": "2025-05-21T16:25:02.577661",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.558062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(data_loader, model, optimizer, criterion, device, save_checkpoints, checkpoint_path, current_epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in tqdm(data_loader, desc=\"Iterating training graphs\", unit=\"batch\"):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "        total += data.y.size(0)\n",
    "\n",
    "    # Save checkpoints if required\n",
    "    if save_checkpoints:\n",
    "        checkpoint_file = f\"{checkpoint_path}_epoch_{current_epoch + 1}.pth\"\n",
    "        torch.save(model.state_dict(), checkpoint_file)\n",
    "        print(f\"Checkpoint saved at {checkpoint_file}\")\n",
    "\n",
    "    return total_loss / len(data_loader),  correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6139b912",
   "metadata": {
    "id": "8peFiIS19ZpK",
    "papermill": {
     "duration": 0.017908,
     "end_time": "2025-05-21T16:25:02.607848",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.589940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(data_loader, model, device, calculate_accuracy=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    total_loss = 0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader, desc=\"Iterating eval graphs\", unit=\"batch\"):\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            \n",
    "            if calculate_accuracy:\n",
    "                correct += (pred == data.y).sum().item()\n",
    "                total += data.y.size(0)\n",
    "                total_loss += criterion(output, data.y).item()\n",
    "            else:\n",
    "                predictions.extend(pred.cpu().numpy())\n",
    "    if calculate_accuracy:\n",
    "        accuracy = correct / total\n",
    "        return  total_loss / len(data_loader),accuracy\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fbdbd871",
   "metadata": {
    "id": "WanuZKxy9Zs-",
    "papermill": {
     "duration": 0.016728,
     "end_time": "2025-05-21T16:25:02.635694",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.618966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_predictions(predictions, test_path):\n",
    "    script_dir = os.getcwd() \n",
    "    submission_folder = os.path.join(script_dir, \"submission\")\n",
    "    test_dir_name = os.path.basename(os.path.dirname(test_path))\n",
    "    \n",
    "    os.makedirs(submission_folder, exist_ok=True)\n",
    "    \n",
    "    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n",
    "    \n",
    "    test_graph_ids = list(range(len(predictions)))\n",
    "    output_df = pd.DataFrame({\n",
    "        \"id\": test_graph_ids,\n",
    "        \"pred\": predictions\n",
    "    })\n",
    "    \n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fc3d24da",
   "metadata": {
    "id": "uyHIJS5U9ZzB",
    "papermill": {
     "duration": 0.017765,
     "end_time": "2025-05-21T16:25:02.664538",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.646773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_training_progress(train_losses, train_accuracies, output_dir):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Training Loss\", color='blue')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss per Epoch')\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color='green')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training Accuracy per Epoch')\n",
    "\n",
    "    # Save plots in the current directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"training_progress.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "22574fa5",
   "metadata": {
    "papermill": {
     "duration": 0.016577,
     "end_time": "2025-05-21T16:25:02.692205",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.675628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_user_input(prompt, default=None, required=False, type_cast=str):\n",
    "\n",
    "    while True:\n",
    "        user_input = input(f\"{prompt} [{default}]: \")\n",
    "        \n",
    "        if user_input == \"\" and required:\n",
    "            print(\"This field is required. Please enter a value.\")\n",
    "            continue\n",
    "        \n",
    "        if user_input == \"\" and default is not None:\n",
    "            return default\n",
    "        \n",
    "        if user_input == \"\" and not required:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            return type_cast(user_input)\n",
    "        except ValueError:\n",
    "            print(f\"Invalid input. Please enter a valid {type_cast.__name__}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "139e88b2",
   "metadata": {
    "papermill": {
     "duration": 0.017703,
     "end_time": "2025-05-21T16:25:02.721184",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.703481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_arguments():\n",
    "    args = {}\n",
    "    args['train_path'] = get_user_input(\"Path to the training dataset (optional)\")\n",
    "    args['test_path'] = get_user_input(\"Path to the test dataset\", required=True)\n",
    "    args['num_checkpoints'] = get_user_input(\"Number of checkpoints to save during training\", type_cast=int)\n",
    "    args['device'] = get_user_input(\"Which GPU to use if any\", default=1, type_cast=int)\n",
    "    args['gnn'] = get_user_input(\"GNN type (gin, gin-virtual, gcn, gcn-virtual)\", default='gin')\n",
    "    args['drop_ratio'] = get_user_input(\"Dropout ratio\", default=0.0, type_cast=float)\n",
    "    args['num_layer'] = get_user_input(\"Number of GNN message passing layers\", default=5, type_cast=int)\n",
    "    args['emb_dim'] = get_user_input(\"Dimensionality of hidden units in GNNs\", default=300, type_cast=int)\n",
    "    args['batch_size'] = get_user_input(\"Input batch size for training\", default=32, type_cast=int)\n",
    "    args['epochs'] = get_user_input(\"Number of epochs to train\", default=10, type_cast=int)\n",
    "    args['baseline_mode'] = get_user_input(\"Baseline mode: 1 (CE), 2 (Noisy CE)\", default=1, type_cast=int)\n",
    "    args['noise_prob'] = get_user_input(\"Noise probability p (used if baseline_mode=2)\", default=0.2, type_cast=float)\n",
    "\n",
    "    \n",
    "    return argparse.Namespace(**args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "45bffa19",
   "metadata": {
    "papermill": {
     "duration": 0.118164,
     "end_time": "2025-05-21T16:25:02.850799",
     "exception": true,
     "start_time": "2025-05-21T16:25:02.732635",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Path to the training dataset (optional) [None]:  datasets/D/train.json.gz\n",
      "Path to the test dataset [None]:  datasets/D/test.json.gz\n",
      "Number of checkpoints to save during training [None]:  10\n",
      "Which GPU to use if any [1]:  1\n",
      "GNN type (gin, gin-virtual, gcn, gcn-virtual) [gin]:  gcn\n",
      "Dropout ratio [0.0]:  0.2\n",
      "Number of GNN message passing layers [5]:  \n",
      "Dimensionality of hidden units in GNNs [300]:  \n",
      "Input batch size for training [32]:  \n",
      "Number of epochs to train [10]:  \n",
      "Baseline mode: 1 (CE), 2 (Noisy CE) [1]:  2\n",
      "Noise probability p (used if baseline_mode=2) [0.2]:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments received:\n",
      "train_path: datasets/D/train.json.gz\n",
      "test_path: datasets/D/test.json.gz\n",
      "num_checkpoints: 10\n",
      "device: 1\n",
      "gnn: gcn\n",
      "drop_ratio: 0.2\n",
      "num_layer: 5\n",
      "emb_dim: 300\n",
      "batch_size: 32\n",
      "epochs: 10\n",
      "baseline_mode: 2\n",
      "noise_prob: 0.2\n"
     ]
    }
   ],
   "source": [
    "def populate_args(args):\n",
    "    print(\"Arguments received:\")\n",
    "    for key, value in vars(args).items():\n",
    "        print(f\"{key}: {value}\")\n",
    "args = get_arguments()\n",
    "populate_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f58745df",
   "metadata": {
    "id": "lHX55XGECXBr",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "script_dir = os.getcwd() \n",
    "# device = torch.device(f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_checkpoints = args.num_checkpoints if args.num_checkpoints else 3\n",
    "    \n",
    "if args.gnn == 'gin':\n",
    "    model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False).to(device)\n",
    "elif args.gnn == 'gin-virtual':\n",
    "    model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True).to(device)\n",
    "elif args.gnn == 'gcn':\n",
    "    model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False).to(device)\n",
    "elif args.gnn == 'gcn-virtual':\n",
    "    model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True).to(device)\n",
    "else:\n",
    "    raise ValueError('Invalid GNN type')\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "if args.baseline_mode == 2:\n",
    "    pass\n",
    "else:\n",
    "    criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6a025914",
   "metadata": {
    "id": "BTYT5jYuChPb",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dir_name = os.path.basename(os.path.dirname(args.test_path))\n",
    "logs_folder = os.path.join(script_dir, \"logs\", test_dir_name)\n",
    "log_file = os.path.join(logs_folder, \"training.log\")\n",
    "os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logging.getLogger().addHandler(logging.StreamHandler())\n",
    "\n",
    "checkpoint_path = os.path.join(script_dir, \"checkpoints\", f\"model_{test_dir_name}_best.pth\")\n",
    "checkpoints_folder = os.path.join(script_dir, \"checkpoints\", test_dir_name)\n",
    "os.makedirs(checkpoints_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dbca6779",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists(checkpoint_path) and not args.train_path:\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    print(f\"Loaded best model from {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ef4a359c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [01:19<00:00,  3.25batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/D/model_D_epoch_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 65/65 [00:15<00:00,  4.31batch/s]\n",
      "Epoch 1/10, Loss: 1.0289, Train Acc: 0.4893, Val Acc: 0.4032\n",
      "Epoch 1/10, Loss: 1.0289, Train Acc: 0.4893, Val Acc: 0.4032\n",
      "Epoch 1/10, Loss: 1.0289, Train Acc: 0.4893, Val Acc: 0.4032\n",
      "Epoch 1/10, Loss: 1.0289, Train Acc: 0.4893, Val Acc: 0.4032\n",
      "Epoch 1/10, Loss: 1.0289, Train Acc: 0.4893, Val Acc: 0.4032\n",
      "Epoch 1/10, Loss: 1.0289, Train Acc: 0.4893, Val Acc: 0.4032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.0289, Train Acc: 0.4893, Val Acc: 0.4032\n",
      "Best model updated and saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/model_D_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [01:40<00:00,  2.56batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/D/model_D_epoch_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 65/65 [00:14<00:00,  4.41batch/s]\n",
      "Epoch 2/10, Loss: 0.7454, Train Acc: 0.6058, Val Acc: 0.2067\n",
      "Epoch 2/10, Loss: 0.7454, Train Acc: 0.6058, Val Acc: 0.2067\n",
      "Epoch 2/10, Loss: 0.7454, Train Acc: 0.6058, Val Acc: 0.2067\n",
      "Epoch 2/10, Loss: 0.7454, Train Acc: 0.6058, Val Acc: 0.2067\n",
      "Epoch 2/10, Loss: 0.7454, Train Acc: 0.6058, Val Acc: 0.2067\n",
      "Epoch 2/10, Loss: 0.7454, Train Acc: 0.6058, Val Acc: 0.2067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.7454, Train Acc: 0.6058, Val Acc: 0.2067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [01:20<00:00,  3.20batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/D/model_D_epoch_3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 65/65 [00:14<00:00,  4.43batch/s]\n",
      "Epoch 3/10, Loss: 0.6427, Train Acc: 0.6425, Val Acc: 0.2899\n",
      "Epoch 3/10, Loss: 0.6427, Train Acc: 0.6425, Val Acc: 0.2899\n",
      "Epoch 3/10, Loss: 0.6427, Train Acc: 0.6425, Val Acc: 0.2899\n",
      "Epoch 3/10, Loss: 0.6427, Train Acc: 0.6425, Val Acc: 0.2899\n",
      "Epoch 3/10, Loss: 0.6427, Train Acc: 0.6425, Val Acc: 0.2899\n",
      "Epoch 3/10, Loss: 0.6427, Train Acc: 0.6425, Val Acc: 0.2899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.6427, Train Acc: 0.6425, Val Acc: 0.2899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [01:21<00:00,  3.16batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/D/model_D_epoch_4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 65/65 [00:15<00:00,  4.15batch/s]\n",
      "Epoch 4/10, Loss: 0.5756, Train Acc: 0.6680, Val Acc: 0.5827\n",
      "Epoch 4/10, Loss: 0.5756, Train Acc: 0.6680, Val Acc: 0.5827\n",
      "Epoch 4/10, Loss: 0.5756, Train Acc: 0.6680, Val Acc: 0.5827\n",
      "Epoch 4/10, Loss: 0.5756, Train Acc: 0.6680, Val Acc: 0.5827\n",
      "Epoch 4/10, Loss: 0.5756, Train Acc: 0.6680, Val Acc: 0.5827\n",
      "Epoch 4/10, Loss: 0.5756, Train Acc: 0.6680, Val Acc: 0.5827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.5756, Train Acc: 0.6680, Val Acc: 0.5827\n",
      "Best model updated and saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/model_D_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [01:20<00:00,  3.21batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/D/model_D_epoch_5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 65/65 [00:15<00:00,  4.27batch/s]\n",
      "Epoch 5/10, Loss: 0.5243, Train Acc: 0.6908, Val Acc: 0.6094\n",
      "Epoch 5/10, Loss: 0.5243, Train Acc: 0.6908, Val Acc: 0.6094\n",
      "Epoch 5/10, Loss: 0.5243, Train Acc: 0.6908, Val Acc: 0.6094\n",
      "Epoch 5/10, Loss: 0.5243, Train Acc: 0.6908, Val Acc: 0.6094\n",
      "Epoch 5/10, Loss: 0.5243, Train Acc: 0.6908, Val Acc: 0.6094\n",
      "Epoch 5/10, Loss: 0.5243, Train Acc: 0.6908, Val Acc: 0.6094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.5243, Train Acc: 0.6908, Val Acc: 0.6094\n",
      "Best model updated and saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/model_D_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [01:20<00:00,  3.18batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/D/model_D_epoch_6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 65/65 [00:14<00:00,  4.42batch/s]\n",
      "Epoch 6/10, Loss: 0.4987, Train Acc: 0.6972, Val Acc: 0.6853\n",
      "Epoch 6/10, Loss: 0.4987, Train Acc: 0.6972, Val Acc: 0.6853\n",
      "Epoch 6/10, Loss: 0.4987, Train Acc: 0.6972, Val Acc: 0.6853\n",
      "Epoch 6/10, Loss: 0.4987, Train Acc: 0.6972, Val Acc: 0.6853\n",
      "Epoch 6/10, Loss: 0.4987, Train Acc: 0.6972, Val Acc: 0.6853\n",
      "Epoch 6/10, Loss: 0.4987, Train Acc: 0.6972, Val Acc: 0.6853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.4987, Train Acc: 0.6972, Val Acc: 0.6853\n",
      "Best model updated and saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/model_D_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [01:24<00:00,  3.04batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/D/model_D_epoch_7.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 65/65 [00:15<00:00,  4.18batch/s]\n",
      "Epoch 7/10, Loss: 0.4823, Train Acc: 0.7112, Val Acc: 0.7091\n",
      "Epoch 7/10, Loss: 0.4823, Train Acc: 0.7112, Val Acc: 0.7091\n",
      "Epoch 7/10, Loss: 0.4823, Train Acc: 0.7112, Val Acc: 0.7091\n",
      "Epoch 7/10, Loss: 0.4823, Train Acc: 0.7112, Val Acc: 0.7091\n",
      "Epoch 7/10, Loss: 0.4823, Train Acc: 0.7112, Val Acc: 0.7091\n",
      "Epoch 7/10, Loss: 0.4823, Train Acc: 0.7112, Val Acc: 0.7091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.4823, Train Acc: 0.7112, Val Acc: 0.7091\n",
      "Best model updated and saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/model_D_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [01:26<00:00,  2.97batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/D/model_D_epoch_8.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 65/65 [00:15<00:00,  4.14batch/s]\n",
      "Epoch 8/10, Loss: 0.4656, Train Acc: 0.7202, Val Acc: 0.7203\n",
      "Epoch 8/10, Loss: 0.4656, Train Acc: 0.7202, Val Acc: 0.7203\n",
      "Epoch 8/10, Loss: 0.4656, Train Acc: 0.7202, Val Acc: 0.7203\n",
      "Epoch 8/10, Loss: 0.4656, Train Acc: 0.7202, Val Acc: 0.7203\n",
      "Epoch 8/10, Loss: 0.4656, Train Acc: 0.7202, Val Acc: 0.7203\n",
      "Epoch 8/10, Loss: 0.4656, Train Acc: 0.7202, Val Acc: 0.7203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.4656, Train Acc: 0.7202, Val Acc: 0.7203\n",
      "Best model updated and saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/model_D_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [01:25<00:00,  3.00batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/D/model_D_epoch_9.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 65/65 [00:15<00:00,  4.07batch/s]\n",
      "Epoch 9/10, Loss: 0.4426, Train Acc: 0.7302, Val Acc: 0.6882\n",
      "Epoch 9/10, Loss: 0.4426, Train Acc: 0.7302, Val Acc: 0.6882\n",
      "Epoch 9/10, Loss: 0.4426, Train Acc: 0.7302, Val Acc: 0.6882\n",
      "Epoch 9/10, Loss: 0.4426, Train Acc: 0.7302, Val Acc: 0.6882\n",
      "Epoch 9/10, Loss: 0.4426, Train Acc: 0.7302, Val Acc: 0.6882\n",
      "Epoch 9/10, Loss: 0.4426, Train Acc: 0.7302, Val Acc: 0.6882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.4426, Train Acc: 0.7302, Val Acc: 0.6882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [01:25<00:00,  3.00batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/onyxia/work/DL-Hackathon/hackaton/checkpoints/D/model_D_epoch_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 65/65 [00:15<00:00,  4.08batch/s]\n",
      "Epoch 10/10, Loss: 0.4214, Train Acc: 0.7336, Val Acc: 0.6590\n",
      "Epoch 10/10, Loss: 0.4214, Train Acc: 0.7336, Val Acc: 0.6590\n",
      "Epoch 10/10, Loss: 0.4214, Train Acc: 0.7336, Val Acc: 0.6590\n",
      "Epoch 10/10, Loss: 0.4214, Train Acc: 0.7336, Val Acc: 0.6590\n",
      "Epoch 10/10, Loss: 0.4214, Train Acc: 0.7336, Val Acc: 0.6590\n",
      "Epoch 10/10, Loss: 0.4214, Train Acc: 0.7336, Val Acc: 0.6590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.4214, Train Acc: 0.7336, Val Acc: 0.6590\n"
     ]
    }
   ],
   "source": [
    "if args.train_path:\n",
    "    full_dataset = GraphDataset(args.train_path, transform=add_zeros)\n",
    "    val_size = int(0.2 * len(full_dataset))\n",
    "    train_size = len(full_dataset) - val_size\n",
    "\n",
    "    if args.baseline_mode == 2:\n",
    "        criterion = GCODLoss(args.noise_prob)\n",
    "    \n",
    "    generator = torch.Generator().manual_seed(12)\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    num_epochs = args.epochs\n",
    "    best_val_accuracy = 0.0   \n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    if num_checkpoints > 1:\n",
    "        checkpoint_intervals = [int((i + 1) * num_epochs / num_checkpoints) for i in range(num_checkpoints)]\n",
    "    else:\n",
    "        checkpoint_intervals = [num_epochs]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train(\n",
    "            train_loader, model, optimizer, criterion, device,\n",
    "            save_checkpoints=(epoch + 1 in checkpoint_intervals),\n",
    "            checkpoint_path=os.path.join(checkpoints_folder, f\"model_{test_dir_name}\"),\n",
    "            current_epoch=epoch\n",
    "        )\n",
    "\n",
    "        val_loss,val_acc = evaluate(val_loader, model, device, calculate_accuracy=True)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        logging.info(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        \n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Best model updated and saved at {checkpoint_path}\")\n",
    "\n",
    "    plot_training_progress(train_losses, train_accuracies, os.path.join(logs_folder, \"plots\"))\n",
    "    plot_training_progress(val_losses, val_accuracies, os.path.join(logs_folder, \"plotsVal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "07594aff",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11328"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del train_dataset\n",
    "del train_loader\n",
    "del full_dataset\n",
    "del val_dataset\n",
    "del val_loader\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ba668fff",
   "metadata": {
    "id": "xsXZIj4Mdu3I",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = GraphDataset(args.test_path, transform=add_zeros)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7828e983",
   "metadata": {
    "id": "x1OnGq_nCmTr",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 71/71 [00:16<00:00,  4.37batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to /home/onyxia/work/DL-Hackathon/hackaton/submission/testset_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "predictions = evaluate(test_loader, model, device, calculate_accuracy=False)\n",
    "save_predictions(predictions, args.test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4f1594ef-0f72-4716-b53f-8da074eae4d5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '/home/onyxia/work/DL-Hackathon/hackaton/logs' has been compressed into '/home/onyxia/work/DL-Hackathon/hackaton/logs.gz'\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "def gzip_folder(folder_path, output_file):\n",
    "    \"\"\"\n",
    "    Compresses an entire folder into a single .tar.gz file.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder to compress.\n",
    "        output_file (str): Path to the output .gz file.\n",
    "    \"\"\"\n",
    "    with tarfile.open(output_file, \"w:gz\") as tar:\n",
    "        tar.add(folder_path, arcname=os.path.basename(folder_path))\n",
    "    print(f\"Folder '{folder_path}' has been compressed into '{output_file}'\")\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"/home/onyxia/work/DL-Hackathon/hackaton/logs\"            # Path to the folder you want to compress\n",
    "output_file = \"/home/onyxia/work/DL-Hackathon/hackaton/logs.gz\"        # Output .gz file name\n",
    "gzip_folder(folder_path, output_file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 108.457758,
   "end_time": "2025-05-21T16:25:04.482169",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-21T16:23:16.024411",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
